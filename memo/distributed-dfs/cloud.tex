\documentclass{note}
\usepackage{mydef,myenv}
\usepackage[T1]{fontenc}
%\usepackage[Lenny]{fncychap}
\begin{document}
\title{\Large\bf{}Notes on Cloud Operating Systems:\\  Workflows, Distributed
  Shared Memory, and Distributed File Systems}
\author{\normalsize{}cjeongkr@gmail.com}
\date{}
\maketitle

%\Large
\small

\section{Introduction}
\bit
\w Old Computer: HW + OS + Application
\w New Computer: distributed HWs + Cloud OS + Cloud Applications
\eit

%
%\part{Workflows: Distributed Processes}
\section{Workflows (Distributed Processes): Overview}
\section{Case Study: MapReduce}

% 
%\part{Distributed Shared Memory}
\section{Distributed Shared Memory: Overview}


%
%\part{Distributed File Systems}
\section{Distributed File Systems: Overview}
\bit
\w distributed file systems:
   \bit
   \w allows multiple process to share data
   \eit
\eit

%
%
%
\section{Case Study: Sun Network File System (NFS)}
\subsection{Overview}
\bit
\w basic idea: \bb{each file server provides a standardized view of its local
  file system} 
\w traditional \bb{client-server}-based
\w not actually a true file system but {\em a collection of protocols that
  together provide clients with a model of a distributed file system\/} 
\eit
\subsection{NFS architecture}
\bit
\w NFS model is \bb{remote file service}
  \bit
  \w \bb{clients} are offered transparent access to a file system which is
  managed by \bb{remote server}
  \w clients are unaware of the actual location of files
  \w {\em clients\/} are offered an interface to a file system similiar to POSIX API
  for files and {\em server\/} implements that interface  
  \eit
\w NFS  is a \bb{remote access model} (cf. \bb{upload/download model})
  \bit
  \w \bb{remote access model}
  \w \bb{upload/download model}: client access a file locally after having
  downloaded it from the server
  \eit
\w nowadays, NFS is incorporated into \bb{VFS (Virtual File System)}, which
provides a unified inteface to many different (local or distributed) file
systems 
\eit

\subsection{Naming in NFS}
\bit
\w through \bb{mounting} remote file system onto its local file system,
transparency is achieved
\w server is said to \bb{export} its directory 
\w \bb{implication!}: {\em clients do NOT share name spaces\/}
   \bit
   \w client A sees a file at \verb+/remote/vu/mbox+
   \w client B sees the same file at \verb+/work/me/mbox+
   \w \bb{file sharing in NFS is hard!} since A and B have different name
   spaces

   \w \bb{solution}: standardize the mount point (e.g. all clients mount
   remote FS into \verb+/nfsmnt+)
   \eit
\eit

%
%
%
\section{Case Study: Coda}


%
%
%
\section{Case Study: Plan 9}

%
%
%
\section{Case Study: xFS}

%
%
%
\section{Case Study: SFS}

%
%
%
\section{Case Study: GFS (Google File System)}
\subsection{Overview}
\bit
\w GFS is a scalable file system for large distributed
data-intensive applications~\cite{Ghemawat03}.
\w design goals (quite conventional):
  \bit
  \w performance
  \w scalability
  \w reliability
  \w availability
  \eit
\w GFS-specific design goals
  \bit
  \w \bb{component failures are the norm rather than the exception}
      \bit
      \w {\em sources of failures\/} are:
       application bug, OS bug, human error, machine failures, network
       failures 
      \w {\em solution\/}: constant monitoring, error detection, fault
      tolerance, automatic recovery
      \eit
  \w \bb{small number of huge files} or \bb{huge number of small files} or
  both 
     \bit
     \w \bb{basic assumption on paramters should be revised}:
        \bit
        \w I/O operations 
        \w buffer size
        \w block size
        \eit
     \eit
  \w \bb{special access patterns}: most files are mutated by appending new data rather than overwriting existing data
    \bit
    \w random writes are practically non-existent
    \w files are mostly sequentially read 
        \bit
        \w \bb{large streaming reads}: 100s of KBs (more commonly 1MBs or more)
        \w \bb{small random reads}: a few KBs at some arbitrary offsets
        \eit
    \eit
  \w \bb{co-designing applications w/ the file system API benefits the overall
    system by increasing flexibility}
  \w \bb{high sustained bandwith is more important than low latency}
  \eit
\eit

\subsection{GFS API}
\bit
\w \bb{create}, \bb{delete}, \bb{open}, \bb{close}, \bb{read}, \bb{write}
\w \bb{snapshot}: creates a copy of a file or directory tree at low cost
\w \bb{record append}: allows multiple clients to append data to the same file
concurrently while guaranteeing the atomicity of each individial client's
append 
   \bit
   \w good for merging outputs to the tempory data file (e.g. in MapReduce)
   \w no extra synchronization (e.g. locking) is required
   \eit
\eit

\subsection{GFS architecture}
\bit
\w a GFS cluster consists of 
   \bit
   \w a signle \bb{master}
   \w multiple \bb{chunkservers}
   \w multiple \bb{clients}
   \eit
\w \bb{files} are divided into fixed-size \bb{chunks}
\w \bb{Master}: 
   \bit
   \w store \bb{metadata} associated with the chunks (e.g. tables mapping the 4-bit
   labels to chunk locations)
   \w periodically receives updates from each chunkserver (``hear-beat
   messages'') 
   \w permission for modificiations are handled by finite-time \bb{leases}
   granted to clients
   \eit
\w \bb{Chunkserver}: 
    \bit
    \w store the files, which each individual file broken up to fixed-size
    chunks (about 64MB)
    \eit
\w \bb{metadata}: there are three types
   \bit
   \w \bb{file and chunk namespaces}: kept persistent
   \w \bb{mapping from files to chunks}: kept persistent
   \w \bb{locations of each chunk's replicas}: master does not store chunk
   location persistently; instead, it askes each chunkserver about its chunks
   at master startup and whenever a chunkserver joins the cluster
   \eit
\w \bb{handling metadata (in-memory)}
   \bit
   \w master periodically scan through its entire state in the background for:
      \bit
      \w  chunk garbage collection
      \w re-replication in the presence of chunkserver failure
      \w chunk migration to balance load and disk space usage
      \eit
   \w 64B metadata for each 64MB chunk
   \eit
\w \bb{chunk location}:
\eit

\subsection{Consistency model}
\bit
\w \bb{file namespace mutation} (e.g. \bb{file creation}) are atomic
\w above is achieved by {\em namespace locking\/} by master, where atomicity
  and correctness is guaranteed 
\eit

%
%
%
\section{Case Study: HDFS (Hadoop File System)}
\subsection{Assumptions and goals}
\bit
\w \bb{hardware failure}
\w \bb{streaming data access}
\w \bb{large data sets}
\w \bb{simple coherency model}: {\em write-once-read-many\/} access model for
files; a file once created, written, and closed need not be changed
\w \bb{``moving computation is cheaper than moving data''}: computatino is
more efficient ``near'' the data
\w \bb{portability across heterogeneous HW and SW platforms}
\eit

\subsection{{\tt{}NameNode} and {\tt{}DataNodes}}
\bit
\w 
\eit

\subsection{File system namespace}


%
%
%
\section{Case Study: IBM GPFS (General Parallel File System)}




\bibliographystyle{plain}
\bibliography{bib/mac,bib/softsys,bib/compsys}

\pagebreak
\tableofcontents
\end{document}
