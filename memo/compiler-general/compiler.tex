\documentclass{myproc}
\usepackage{theorem,mydef,amssymb,newalg,myenv}
\usepackage[all]{xy}
%\CompileMatrices

\def\lpre{\ \cdot\!\!>}
\def\rpre{<\!\!\cdot\ }
\def\eqpre{\stackrel{\cdot}{=}}

\begin{document}

\scriptsize
\title{\large\bf{}Notes on Compilers}
%\author{Á¤ Ã¶ÁÖ}
\maketitle

%\chapter{Compilers}
\section{Elements of Language Theory}
\paragraph{Grammars}
\bit
\w A {\bf{}grammar} is a $4$-tuple $(N, T, P, S)$ where
	\ben
	\w [(a)] $N$ is a finite set of {\bf{}nonterminal symbols}
		(a.k.a. {\em{}variables\/} or {\em{}syntactic categories\/}).
	\w [(b)] $T$ is a finite set of {\bf{}terminal symbols}.
		disjoint from $N$.
	\w [(c)] $P$ is a finite subset of 
		\[ (N \cup T)^*N(N \cup T)^* \times (N \cup T)^*, \]
		An element $(\alpha, \beta) \in P$ will be written 
		$\alpha \rightarrow \beta$ and called a {\bf{}production}.
	\w [(d)] $S$ is a distinguished symbol in $N$ call the {\bf{}start
		symbol} (a.k.a. {\em{}sentence symbol\/}).
	\een
\w A grammar defines a language in a recursive manner.
\w A {\bf{}sentential form} of a grammar $G = (N, T, P, S)$ is defined
	as follows:
	\ben
	\w [(a)] $S$ is a sentential form.
	\w [(b)] If $\alpha\beta\gamma$ is a sentential form and
		$\beta \rightarrow \delta \in P$, then
		$\alpha\delta\gamma$ is also a sentential form.
	\een
\w A sentential form with no nonterminal symbols is called a
	{\bf{}sentence} generated by $G$.
\w The {\bf{}language generated by a grammar} $G$, denoted $L(G)$, is
	the set of sentences generated by $G$.
\w We define a relation $\Ra_G$ (read as {\bf{}directly derives})
	as follows:
	If $\alpha\beta\gamma$ is a string in $(N \cup T)^*$
	and $\beta \rightarrow \delta$ is a production in $P$, then
	$\alpha\beta\gamma \Rightarrow_G \alpha\delta\gamma$.
	\bit
	\w $\Ra^+_G$ is the transitive closure of $\Ra_G$.
	\w $\Ra^*_G$ is the reflexive and transitive closure of $\Ra_G$.
	\w $\Ra^k_G$ is the $k$-fold product of $\Ra_G$.
	\w $\Ra_G$ is usually written as $\Ra$ when $G$ is clear from
		the context.
	\eit
\w $L(G) = \{w : w \in T^* \mbox{\ and\ } S \Rightarrow^* w\}$.
\eit

\paragraph{Restricted grammars}
\bit
\w Let $G = (N, T, P, S)$ be a grammar. $G$ is said to be
	\ben
	\w {\bf{}right-linear} if each production in $P$ is of the form
		$A \rightarrow xB$ or $A \rightarrow x$ where
		$A, B \in N$ and $x \in T^*$.
	\w {\bf{}context-free} if each production in $P$ is of the form
		$A \rightarrow \alpha$, where
		$A \in N$ and $\alpha \in (N \cup T)^*)$.\
	\w {\bf{}context-sensitive} if each production in $P$ is of the
		form $\alpha \rightarrow \beta$, where $|\alpha| \le
		|\beta|$.
	\w {\bf{}unrestricted} if $G$ has no restrictions given above.
	\een
\eit

\section{Lexical Analysis}
\bit
\w The set of strings for which the same token is produced is 
	described by a rule called a {\bf{}pattern}. The pattern
	is said to {\em{}match\/} each string in the set.
	A {\bf{}lexeme} is a sequence of characters in the program that is
	matched by the pattern for a token.
\w {\bf{}Tokens} can be treated as terminal symbols in the grammar.
	
\w For some tokens 
	(e.g., identifiers, character strings, integer constants et al.)
	we need to store {\bf{}attributes for tokens} together with the token
	itself.
	\ben
	\w [(a)] {\em{}a pointer to the symbol-table entry\/}
	\w [(b)] {\em{}lexeme\/} for an identifier and {\em{}line number\/}
		for diagnostic purposes
	\w [(c)] integer-values for `number' tokens 
	\w [(d)] character-strings for `string' tokens
	\een
\eit

\paragraph{Input Buffering}
\bit
\w Background: there are times when the lexical analyzer needs to look
	ahead several characters beyond the lexeme for a pattern
	before a match can be announced.
\w {\bf{}Buffer pairs}
	\bit
	\w We use a buffer divided into two $N$-character halves, where
		$N$ is the size of one {\em{}disk block\/}.
	\w \verb+lexeme_beginning+  and
		\verb+forward+ pointer
	\w The string of characters between the two pointers is the
		current lexeme.
	\w 2 tests are needed for ``end of buffers''
	\eit
\w {\bf{}Sentinels}
	\bit
	\w Extend each buffer half to hold a {\em{}sentinel\/} character.
	\w Only 1 test is needed for ``end of buffer.''
	\eit
\eit
\paragraph{Specification of Tokens}
\bit
\w {\bf{}Regular expressions} are a sufficient notation for specifying
	patterns in lexical analyzers.
\w Operations on ``languages''
	\bit
	\w {\bf{}union} of $L$ and $M$, denoted by $L \cup M$
	\w {\bf{}concatenation} of $L$ and $M$, denoted by $LM$
	\w {\bf{}Kleene closure} of $L$, denoted by $L^*$
	\w {\bf{}positive closure} of $L$, denoted by $L^+$
	\eit
\w Algebraic properties of regular expressions
	\ben
	\w [(a)] $r|s = s|r$ \ (commutativity of $|$)
	\w [(b)] $r|(s|t) = (r|s)|t$ \ (associativity of $|$)
	\w [(c)] $(rs)t = r(st)$ \ (associativity of $\cdot$)
	\w [(d)] $r(s|t) = rs|rt, (s|t)r = (sr|tr)$ \
		($\cdot$ distributes over $|$)
	\w [(e)] $\epsilon{r} = r\epsilon = r$ \ ($\epsilon$ is an 
		identity element for $\cdot$)
	\w [(f)] $r^* = (r|\epsilon)^*$
	\w [(g)] $(r^*)^* = r^*$ \ (idempotency of $*$)
	\een
\w Given an alphabet $\Sigma$,
	a {\bf{}regular definition} is a sequence of definitions of the form
	\[ d_1 \rightarrow r_1,  \cdots, d_n \rightarrow r_n\]
	where each $d_i$ is a distinct {\em{}name\/}, and each $r_i$
	is a regular expression over the symbols in 
	$\Sigma \cup \{d_1, \cdots,d_{i-1}\}$
	\bit
	\w We can give {\sl\bfseries{}names} to regular expressions 
		in regular definition.
	\eit
\w ({\bf{}Nonregular Sets}) Regular expressions cannot be used to 
	describe {\em{}balanced or nested constructs\/} or 
	{\em{}repeating strings\/} such as 
	\[\{wcw: \mbox{$w$ is a string of
	$a$'s and $b$'s}\} \]
	\bit
	\w Balanced or nested constructs can be described by 
		{\em{}context-free grammars\/}.
	\w Repeating strings cannot be described by {\em{}context-free
		grammars\/}.
	\w Regular expressions can be used to denote only {\bf{}fixed 
		number of repetitions} or {\bf{}an unspecified number of
		repetitions} of a given construct.
	\eit
\eit
\paragraph{Recognition of  Tokens}
\bit
\w A simple technique for separating keywords from identifiers:
	initialize appropriately the symbol table in which information
	about identifiers are saved.
\w Lex, a lexical analyzer generator
	\bit
	\w {\bf{}Lookahead operator} $r_1/r_2$ matches a string
		in $r_1$ but only if followed by a string in $r_2$
		\bit
		\w $r_2$ indicates the right context for a match.
		\w Fortran statements
		\begin{verbatim}
                DO 5 I = 1.25
                DO 5 I = 1,25
		\end{verbatim}
		needs lookahead operators, used as
		\[ \mbox{\tt{}DO}/(\{\mbox{letter}\} | 
			\{\mbox{digit}\})^* \mbox{\tt{}=} (\{\mbox{letter}\} | 
			\{\mbox{digit}\})^*\mbox{\tt{},} \] 
		\eit
	\eit
\eit
\paragraph{Finite Automata}
\bit
\w A {\bf{}recognizer} for a language $L$ 
	is a machine that takes as input a string $x$ and outputs 
	``yes'' if $x \in L$ and ``no'' otherwise.
\w Strategy for building lexical analyzers: build a regular expression
	and construct an equivalent deterministic finite automaton for it.
\w ({\bf{}Nondeterministic Finite Automata}) An NFA is 
	defined to be a $5$-tuple $(S, \Sigma$, $\delta, s_0, F)$
	\bit
	\w NFA {\bf{}accepts} an input string $x$ iff
		there is some path in the transition graph from the
		start state to some accepting state.
	\w A {\bf{}language defined by} an NFA is the set of input strings
		it accepts.
	\eit
\w ({\bf{}Deterministic Finite Automata})
	DFA is a special-case of NFA where\footnote{The definitions of 
	NFA and DFA in \cite{ASU86} is quite different from those of
	\cite{HU79}. First, note that
	\cite{ASU86}'s definition of NFA is a NFA with $\epsilon$-move
	in \cite{HU79}. Second, \cite{HU79} defines $\delta$ to be a
	{\em{}total function\/} while \cite{ASU86} does not.}
	\bit
	\w [(a)] no state has an $\epsilon$-transition
	\w [(b)] for each state $s$ and input symbol $a$, there is 
		{\em{}at most\/}
		one edge labeled $a$ leaving $s$
	\eit
\w {\bf{}Conversion of an NFA into a DFA}:
	\bit
	\w {\sl\bfseries{}$\epsilon\mbox{-CLOSURE}$  computation}
		\vspace{0.2cm}

\begin{minipage}[t]{20cm}
\scriptsize
\begin{algorithm}{$\epsilon$-CLOSURE}{T \in 2^S}
\mbox{push all states in $T$ onto $Stack$}\\
\mbox{initialize $\epsilon$-CLOSURE($T$) to $T$} \\
\begin{WHILE}{\mbox{$Stack$ is not empty}}
	\mbox{pop $t$ off of $Stack$}\\
	\begin{FOR}{\mbox{each state $u$ s.t. $t \stackrel{\epsilon}{\ra} u$}}
		\begin{IF}{\mbox{$u$ is not in $\epsilon$-CLOSURE(T)}}
			\mbox{add $u$ to $\epsilon$-CLOSURE(T)}\\
			\mbox{push $u$ onto $Stack$}
		\end{IF}
	\end{FOR}
\end{WHILE}
\end{algorithm}
\end{minipage}

	\w {\sl\bfseries{}Subset construction algorithm} \vspace{0.2cm}

\begin{minipage}[b]{20cm}
\scriptsize
\begin{algorithm}{SubsetConst}{(S, \Sigma, \delta, s_0, F)}
\mbox{\tt{}// Let $\epsilon$-{\rm{}CLOSURE}$(s_0)$ be the only state}\\
\mbox{\tt{}// in $Dstates$ and it's unmarked}\\
\begin{WHILE}{\mbox{there is an unmarked state $T$ in $Dstates$}}
	\mbox{mark $T$}\\
	\begin{FOR}{\mbox{each input symbol $a$}}
		U \= \epsilon\mbox{-CLOSURE}(\delta(T, a))\\
		\begin{IF}{U \not\in Dstates}
		\mbox{add $U$ as an unmarked state to $Dstates$}
		\end{IF}\\
		Dtran[T, a] \= U
	\end{FOR}
\end{WHILE}
\end{algorithm}
\end{minipage}

	\eit
\w {\bf{}Construction of an NFA from a regular expression}
	\bit
	\w Thompson's construction
	\eit
\w {\bf{}Two-stack simulation of an NFA}: Given an NFA $M$
	and an input string
	$x$ determine if $M$ accepts $x$
	\bit
	\w {\sc{}SimulateNFA} runs in $O(|S|\times|x|)$-time
		\vspace{0.2cm}

\begin{minipage}[b]{20cm}
\scriptsize
\begin{algorithm}{SimulateNFA}{M = (S, \Sigma, \delta, s_0, F), x}
S \= \epsilon\mbox{-CLOSURE}(\{s_0\})\\
a \= nextchar\\
\begin{WHILE}{a \not= \mbox{EOF}}
	S \= \epsilon\mbox{-CLOSURE}(\delta(S, a))\\
	a \= nextchar
\end{WHILE}\\
\begin{IF}{S \cap F \ne \emptyset}
	\mbox{return ``YES''}
\ELSE
	\mbox{return ``NO''}
\end{IF}
\end{algorithm}
\end{minipage} 

	\eit
\w {\sl\bfseries{}Time-space tradeoffs\/}:
	given a regular expression $r$ and an input string $x$,
	we have three methods for determining whether $x \in L(r)$;
	same problem encountered in UNIX {\tt{}grep} command implementation
	\ben
	\w [(a)] Construct an NFA $N$ from $r$ and determine whether
		$N$ accepts $x$.
		\bit
		\w NFA construction in $O(|r|)$ time (Thompson's construction)
		\w $N$ has at most $2|r|$ states and $4|r|$ transitions, 
			so transition table takes $O(|r|)$ space
		\w determining whether $N$ accepts $x$ takes $O(|r|\times|x|)$
			time
		\eit
	\w [(b)] Construct a DFA from $r$ by Thompson's construction and
		then subset construction to the resulting NFA
		\bit
		\w Though, in {\sc{}SimulateNFA}, the single iteration
			of the {\bf{}while} loop requires the computation
			of $\delta(S, a)$ for some {\em{}set} $S$, but
			in DFA simulation, the {\bf{}while} loop
			just requires the computation of 
			$\delta(s, a)$ for single state $s$.
		\w The {\em{}space explosion\/} may occur.
		\eit
	\w [(c)] Use a DFA, but avoid 
		constructing all the transition table by using
		a technique called ``{\bf{}lazy transition evaluation}''
		\bit
		\w Transitions are computed at run time but a transition
			from a given state on a given character is
			not determined until it is actually needed.
		\w A cache $C$ is needed.
		\eit

\vspace{0.2cm}

	\centerline{\begin{tabular}{c|c|c} \hline
	Automaton & Space & Time \\ \hline
	NFA & $O(|r|)$ & $O(|r|\times|x|)$ \\
	DFA & $O(2^{|r|})$ & $O(|x|)$ \\ 
	Lazy DFA & $O(|r| + |C|)$ & $O(|x|)$ \\ \hline
		    \end{tabular}}

	\een
\eit

\paragraph{Design of A Lexical Analyzer Generator}
\bit
\w {\bf{}Problem}: to construct a recognizer that looks for 
	lexemes in the input buffer given a Lex specification of the
	form \[\arc{(r_i, action_i): 1 \le i \le k}\]
\w {\bf{}NFA-based solution}
	\bit
	\w Algorithm:
		\ben
		\w [(a)] generate an NFA, $N(r_i)$ for each pattern $r_i$
		\w [(b)] add a new start state $s_0$ and link $s_0$ to the
			start state of each $N(r_i)$ with an 
			$\epsilon$-transition; this is an NFA for
			language $L(r_1 \mid r_2 \cdots \mid r_k)$
		\w [(c)] execute {\sc{}SimulateNFA}, a version modified for 
			{\em{}longest-match\/} and {\em{}first-line-match\/}
		\een
	\w For the {\sl\bfseries{}longest-match\/}, even if we find a 
		set of states
		containing an accepting state, we must continue
		to simulate the NFA until it reaches 
		{\em{}termination\/}, i.e., a set of states
		from which there are no transitions on the current
		input symbol\footnote{There's an assumption that
		there's a restriction on the length of an identifier
		so that a source program may not fill the entire input buffer
		without having the NFA reach termination.}.
	\w Whenever we add an accepting state to the current set of states,
		record the current input position and the pattern $r_i$ 
		corresponding to this accepting state.
	\w Upon termination, we retract the forward pointer to the position
		at which the last match occurred.
		
	\eit
\w {\bf{}DFA-based solution}
	\bit
	\w Algorithm:
		\ben
		\w [(a)] Convert the NFA to a DFA using the subset
			construction algorithm
		\een
	\w Since $\delta$ in a DFA is a total function, detecting
		{\em{}termination\/} is different from the NFA case:
		now termination means the state, $[]$,  which has
		only self-loop transitions; note that $[s_0, s_1]$
		in the DFA corresponds to the set of states $\{s_0, s_1\}$
		of the NFA.
	\w For linear time longest-match tokenization, refer to
		\cite{Reps98}.
	\eit
\w {\bf{}Implementation of the look-ahead operator}: given a pattern
	$r_1 / r_2$, convert the pattern to an NFA {\em{}treating
	$/$ as if it were $\epsilon$\/}. After acceptance rollback
	to the state just before `the' $\epsilon$-transition.
	
\eit


\paragraph{Constructing Efficient DFA-Based Pattern Matchers from Regular
	Expressions}
\bit
\w {\bf{}Outline}: 
	\[ \xymatrix@+5.0pc{
	**{[F]}{\mbox{RegEx}} 
		\ar[r]^{\mbox{\footnotesize{}Thompson's const.}} &
	**{[F]}{\mbox{NFA}}
		\ar[r]^{\mbox{\footnotesize{}subset const.}} & 
	**{[F]}{\mbox{DFA}}
	}\]
\w A state of an NFA is called {\bf{}important} if it has a non-$\epsilon$
	out-transition.
\w During the subset construction, two subsets
	are {\em{}equivalent} if they have the same important sets,
	and neither or both include accepting state of the NFA.
\w {\bf{}Augmented regular expressions}
	\bit
	\w Problem: resulting NFA's accepting state is not important.
	\w Solution: add a right-end marker \# to $r$ and use the
		{\em{}augmented regular expression\/} $r$\#.
		When the construction is complete, any DFA state with
		a transition on \# is an accepting state.
	\eit
\w {\bf{}From a (augmented) regular expression to a DFA}
	\bit
	\w {\bf{}Algorithm}
		\ben
		\w [(a)] Construct a syntax tree $T$ for $(r)$\#.
		\w [(b)] Compute four functions: 
			{\em{}nullable, firstpos, lastpos,
			followpos\/} by making depth-first 
			traversals over $T$.
		\w [(c)] Build a DFA from {\em{}followpos\/} through
			{\sc{}ConstructDFA} procedure.
		\een
	\w {\bf{}Syntax tree} for $(a|b)^*abb$\#.
	\bit
	\w Each node is labeled with a symbol and its unique
		{\bf{}position}.
\[ \xymatrix@-1.7pc{
	 & & & & & **{=<1.0pc>[o][F]}{\bullet} \ar@{-}[dr] \ar@{-}[dl] & \\
	 & & & & **{=<1.0pc>[o][F]}{\bullet}
		\ar@{-}[dr] \ar@{-}[dl] & & \mbox{\#\ }(6)\\
	 & & & **{=<1.0pc>[o][F]}{\bullet}
		\ar@{-}[dr] \ar@{-}[dl] &  & b(5) & \\
	 & & **{=<1.0pc>[o][F]}{\bullet}
		\ar@{-}[dr] \ar@{-}[dl] &   & b(4) & & \\
	 & **{=<1.0pc>[o][F]}{\ast} \ar@{-}[d] &   & a(3)  &  & & \\
	 &  **{=<1.0pc>[o][F]}{|} \ar@{-}[dr] \ar@{-}[dl]  & &   &  & & \\
	 a(1) &  & b(2)  &   &  & & 
}\]

	\w {\sl\bfseries{}We want to build the DFA whose states
		correspond to sets of positions in the tree\/}.
	\w $\epsilon$-transition obtained from Thompson's construction
		{\em{}encodes the information on {\bfseries{}when
		one position can follow another\/}}.
	\eit
\w Four functions:
	\bit
	\w {\sl\bfseries{}followpos\/}$(i)$:
		the set of positions $j$ s.t. there's some string 
		$\cdots{cd}\cdots$
		such that $i$ corresponds to this occurrence of 
		$c$ and $j$ to this occurrence of $d$.
	\w {\sl\bfseries{}nullable\/}$(n)$:
		{\bf{}True} iff $n$ is the root of subexpression that generate 
		a language that include the empty string
	\w {\sl\bfseries{}firstpos\/}$(n)$:
		the set of positions that can match the first symbol of a 
		string generated by the subexpression rooted at $n$
	\w {\sl\bfseries{}lastpos\/}$(n)$:
		the set of positions that can match the last symbol of a 
		string generated by the subexpression rooted at $n$
	\eit
\w Computing {\em{}followpos\/} and {\em{}nullable}
	\vspace{0.3cm}

		{\scriptsize
		\centerline{\begin{tabular}{p{2.3cm}|p{2.0cm}|p{3.7cm}}\hline
		Node $n$ & $nullable(n)$ & {\em{}firstpos\/}$(n)$ \\ \hline
		$n$ is a leaf labeled	$\epsilon$ &
			{\bf{}True} & $\emptyset$  \\ \hline
		$n$ is a leaf with position $i$ &
			{\bf{}False} & $\{i\}$  \\ \hline
		$\xymatrix@-1.7pc{& | (n) \ar@{-}[dr] \ar@{-}[dl] 
			& \\ c_1 & & c_2}$ 
			& $nullable(c_1)$ {\bf{}or} $nullable(c_2)$ & 
			$\mbox{\em{}firstpos\/}(c_1) \cup 
			\mbox{\em{}firstpos\/}(c_2)$ \\ \hline
		$\xymatrix@-1.7pc{& \bullet (n) \ar@{-}[dr] \ar@{-}[dl] 
			& \\ c_1 & & c_2}$ 
			& $nullable(c_1)$ {\bf{}or} $nullable(c_2)$ & 
			{\bf{}if} $nullable(c_1)$ {\bf{}then}
			$\mbox{\em{}firstpos\/}(c_1) \cup 
			\mbox{\em{}firstpos\/}(c_2)$ {\bf{}else}
			$\mbox{\em{}firstpos\/}(c_1)$ \\ \hline
		$\xymatrix@-1.7pc{& \ast (n) \ar@{-}[dr] \ar@{-}[dl] 
			& \\ c_1 & & c_2}$ 
			& {\bf{}True} &
			$\mbox{\em{}firstpos\/}(c_1)$ \\ \hline
		\end{tabular}}}

		\vspace{0.2cm}

\w Computing {\em{}lastpos\/}: same as {\em{}firstpos\/} except that
	the roles of $c_1$ and $c_2$ are reversed
\w Computing {\em{}followpos\/}$(i)$
	\bit
	\w If $n$ is a cat-node with left child $c_1$ and right child
		$c_2$, and $i$ is a position in $lastpos(c_1)$, 
		then all positions
		in {\em{}firstpos}$(c_2)$ is in {\em{}followpos}$(i)$.
	\w If $n$ is a star-node, and $i$ is a position in  $lastpos(n)$, then
		all positions in {\em{}firstpos}$(n)$ are in
		{\em{}followpos}$(i)$.
	\eit
\w {\sl\bfseries{}How to construct a DFA} $M = (S, \Sigma, \delta, s_0, F)$
	\ben
	\w Let $sym_j$ be the symbol at position $j$.
	\w $S$: a set of distinct {\em{}followpos\/}'s
	\w $\delta(s, a) = t$
		iff $t \in \mbox{\em{}followpos\/}(s)$ and $a = sym_t$
	\w $s_0$: all positions in {\em{}firstpos} of the root
	\w $F$: the position associated with \#
	\een

\eit
\begin{minipage}[b]{20cm}
\scriptsize
\begin{algorithm}{ConstructDFA}{}
\mbox{\tt{}// Initially, the only unmarked state in}\\
\mbox{\tt{}// $Dstates$ is {\rm\em{}firstpos\/}$(root)$
	where $root = (r)$\#}\\
\begin{WHILE}{\mbox{there is an unmarked state $T$ in $Dstates$}}
	\mbox{mark $T$}\\
	\begin{FOR}{\mbox{each input symbol $a$}}
	U \= \{x \in \mbox{\em{}followpos}(p):
		(\exists{p \in T})[\mbox{symbol at position $p$ is $a$}]\}\\
	\end{FOR}
	\begin{IF}{U \ne \empty \mbox{\ and\ } U \not\in Dstates}
		\mbox{add $U$ as an unmarked state to $Dstates$}\\
	\end{IF}
	Dtran[T, a] = U
\end{WHILE}
\end{algorithm}
\end{minipage} 

\eit








\paragraph{Minimizing the Number of State of a DFA}
\bit
\w A string $w$ {\bf{}distinguishes} state $s$ from state $t$ if, by starting
	with the DFA $M$ in state $s$ and feeding it $w$, we end up in an 
	accepting state, but starting in state $t$ and feeding it $w$,
	we end up in a nonaccepting state, or vice versa.
	\bit
	\w $\epsilon$ distinguishes any nonaccepting state from any accepting
		state.
	\eit
\w {\bf{}Idea}: 
	find all groups of states that can be distinguished by some input
	string.
\w At first there are two groups of states: the group of accepting states
	and the group of nonaccepting states. Run the procedure similar
	to {\bf{}Kanellakis-Smolka algorithm} until no more {\em{}splitting\/}
	is possible.
\w ({\bf{}State minimization in lexical analyzers})
	To apply the state minimization procedure to the DFA's constructed 
	through subset construction, we begin the algorithm
	with an initial partition that places in different groups
	all states indicating different tokens.
	
\eit






\section{Syntax Analysis}
\paragraph{The Role of The Parser}
\bit
\w There are three types of parsers for grammars.
	{\bf{}Universal parsing methods} such as {\em{}Cocke-Younger-Kasami 
	algorithm\/} and {\em{}Earley's algorithm\/} can parse
	any context-free grammar but too inefficient.
	Instead,  {\bf{}top-down} or {\bf{}bottom-up} parsing 
	methods (e.g., LL and LR) are used in practice.
	\bit
	\w LL is useful for building handwritten parsers, and the larger
		class of LR allows us the automated tools.
	\eit
\w {\bf{}Error handling}
	\bit
	\w LL and LR methods have the {\bf{}viable-prefix property},
		meaning that they detect that an error has occurred
		as soon as they see a prefix of the input that is not
		a prefix of any string in the language.
		
	\eit
\eit

\paragraph{Context-Free Grammars}
\bit
\w A {\bf{}grammar symbol} is either a nonterminal or a terminal, where
	a {\bf{}nonterminal} is a syntactic variable denoting a set
	of strings and a {\bf{}terminal} is a basic symbol from which
	strings are formed.
\w A {\bf{}production} is of the form $A \rightarrow \alpha$, indicating
	that there is a single nonterminal $A$ on the left of the arrow
	and a string of grammar symbols $\alpha$ to the right of the arrow.
	\bit
	\w If $A \rightarrow \alpha_1, \cdots, A \rightarrow \alpha_k$ are 
		all production with $A$ on the left ($A$-productions), we may
		write $A \rightarrow \alpha_1|\cdots|\alpha_k$. 
		We call $\alpha_1, \cdots, \alpha_k$ the 
		{\bf{}alternatives} for $A$.
	\w A production can be thought of as a {\sl\bfseries{}rewriting rule}.
	\eit
\w We say that $\alpha{A}\beta \Rightarrow \alpha\gamma\beta$ if 
	$A \rightarrow \gamma$ is a production and $\alpha, \beta$ are
	arbitrary\footnote{That's why it's context-free!}
	strings of grammar symbols.
	\bit
	\w We say that $\alpha{A}\beta$ {\bf{}derives} $\alpha\gamma\beta$.
	\w $\Rightarrow$ means the one-step derivation and 
		$\stackrel{\ast}{\Rightarrow}$, the reflexive transitive 
		closure	of $\Rightarrow$, means the 
		zero-or-more-step derivation.
	\eit
\w Given a grammar $G$ with start symbol $S$, 
	the {\bf{}language generated by $G$} is
	$\{w: S \stackrel{+}{\Ra} w\}$.
	\bit
	\w A string of terminals $w$ is in $L(G)$ iff 
		$S \stackrel{+}{\Ra} w$. The string $w$ is called
		a {\bf{}sentence} of $G$.
	\eit
\w A language that can be generated by a context-free grammar is 
	said to be a {\bf{}context-free language}.
\w If $S \stackrel{\ast}{\Ra} \alpha$, where $\alpha$ may contain
	{\em{}nonterminals\/}, then we say that $\alpha$ is 
	a {\bf{}sentential form} of $G$.
\w At each step in derivation, there two choices to be made:
	\ben
	\w [(a)] on which nonterminal to replace
	\w [(b)] on which alternative to use for that nonterminal
	\een
\w A {\bf{}leftmost derivation} is one in which {\em{}the 
	leftmost nonterminal
	of the sentential form is replaced\/} and this is written
	as $\alpha \Rightarrow_{lm} \beta$.
	\bit
	\w If $S \stackrel{\ast}{\Ra}_{lm} \alpha$, then we say 
		$\alpha$ is a {\bf{}left-sentential form} of the grammar
		at hand.
	\w Analogous definitions hold for {\bf{}rightmost derivations}
		(or {\bf{}canonical} derivations).
	\eit
\w {\bf{}Parse trees}
	\bit
	\w A parse tree may be viewed as a graphical representation for
		a derivation that filters out the choice regarding
		replacement order.
	\w The leaves of the parse trees are labeled by nonterminals or
		terminals and, read from left to right, they
		constitute a sentential form, called the {\bf{}yield}
		or {\bf{}frontier} of the tree.
	\eit
\w Every parse tree has associated with it a unique leftmost derivation
	and a unique rightmost derivation.
	\bit
	\w However, we should not assume that every sentence necessarily
		has only one parse tree or only one leftmost or rightmost
		derivation.
	\eit
\w A grammar that produces more than one parse tree for some sentence
	is said to be {\bf{}ambiguous}.
\eit


\paragraph{Writing a Grammar}
\bit
\w With context-free grammars, we cannot describe {\em{}semantic\/}
	constraints such as the fact that a identifier should be
	declared before its  use.
	\bit
	\w Thus the sequences of tokens accepted by a context-free grammar
		from a superset of valid programs.
	\eit
\w Every construct described by a regular expression can be described
	by a context-free grammar.
	\bit
	\w First, construct an NFA from the regular expression.
	\w For each state $i$ of the NFA, create a nonterminal symbol
		$A_i$.
	\w If state $i$ has a transition to $j$ on symbol $a$, introduce
		the production $A_i \rightarrow aA_j$.
	\w If state $i$ is an accepting state, introduce $A_i \rightarrow
		\epsilon$.
	\eit
\w {\bf{}Verifying the language generated by a grammar}
	\bit
	\w A proof that a grammar $G$ generates a language $L$ has two
		parts: 
		\ben
		\w [(a)] Show that every string generated by $G$ in $L$,
			i.e., $L(G) \subseteq L$.
		\w [(b)] Show that every string in $L$ can be generated by
			$G$,
			i.e., $L \subseteq L(G)$.
		\een
	\w Example: 
		\bit
		\w $G: \ S \rightarrow (S)S \mid \epsilon$
		\w $L$: the set of all balanced parentheses
		\w Prove that $L(G) = L$ using induction on the length of
			the string.
		\eit
	\eit
\w {\bf{}Eliminating Ambiguity}
	\bit
	\w {\em{}An ambiguous grammar\/}
	{\scriptsize
	\begin{eqnarray*}
	stmt & \rightarrow & \mbox{\bf{}if\ } expr \mbox{\ \bf{}then\ } stmt\\
		& | & \mbox{\bf{}if\ } expr \mbox{\ \bf{}then\ } stmt
			\mbox{\ \bf{}else\ } stmt\\
		& | & \mbox{\bf{}other}
	\end{eqnarray*}}
	\w ``{\bf{}Dangling-else problem\/}'': 
		two ways to parse the statement
		\[ \mbox{\bf{}if $E_1$ then 
		if $E_2$ then $S_1$ \underline{else} $S_2$}\]
	\w {\bf{}Solution}: 
		Use {\em{}disambiguating rule\/};
			match each {\bf{}else} with the closest previously
			unmatched {\bf{}then}.
	\w {\em{}An unambiguous grammar\/}
	{\scriptsize
	\begin{eqnarray*}
	stmt & \rightarrow & matched\_{stmt}\\
		& | & unmatched\_{stmt}\\
	matched\_{stmt} & \rightarrow & 
		\mbox{\bf{}if\ } expr \mbox{\ \bf{}then\ } 
			matched\_stmt 
	\mbox{\ \bf{}else\ } matched\_stmt\\
		& | & \mbox{\bf{}other}\\
%	& & \mbox{\hspace{1.2cm} \bf{}else\ } matched\_stmt\\
%		& | & \mbox{\bf{}other}\\
	unmatched\_{stmt} & \rightarrow & 
		\mbox{\bf{}if\ } expr \mbox{\ \bf{}then\ } 
			stmt\\
		& | & \mbox{\bf{}if\ } expr \mbox{\ \bf{}then\ } 
			matched\_stmt 
		\mbox{\ \bf{}else\ } unmatched\_stmt
%		& & \mbox{\hspace{1.2cm} \bf{}else\ } unmatched\_stmt
	\end{eqnarray*}}
	
	\eit
\w A grammar is {\bf{}left recursive} if it has a nonterminal $A$ s.t.
	there is a derivation $A \stackrel{+}{\Rightarrow} A\alpha$
	for some string $\alpha$.
\w {\em{}Top-down parsing methods 
	cannot handle left-recursive grammars\/}.
\w {\bf{}Removing immediate left recursion}
	\bit
	\w Case 1)
		$A \rightarrow A\alpha \mid \beta$
	\begin{eqnarray*}
	A & \rightarrow & \beta{}A' \\
	A' & \rightarrow & \alpha{A'} \mid \epsilon
	\end{eqnarray*}
	\w Case 2)
	$A \rightarrow A\alpha_1 \mid \cdots \mid A\alpha_m \mid
		\beta_1 \mid \cdots \mid \beta_n$
	\begin{eqnarray*}
	A & \rightarrow & \beta_1{A'} \mid \cdots \beta_n{A'} \\
	A' & \rightarrow & \alpha_1{A'} \mid \cdots \mid \alpha_m{A'} \mid
		\epsilon
	\end{eqnarray*}
	\eit
\w {\bf{}Eliminating left recursion involving $\ge 1$ steps of derivations}
	
	\vspace{0.2cm}

	\begin{minipage}[b]{20cm}
	\scriptsize
	\begin{algorithm}{EliminateLeftRecursion}{}
	\mbox{arrange the nonterminals in some order $A_1, \cdots, A_n$}\\
	\begin{FOR}{i \= 1 \TO n}
		\begin{FOR}{j \= 1 \TO i-1}
		\mbox{replace $A_i \rightarrow A_j\gamma$ by
			$A_i \rightarrow \delta_1\gamma \mid \cdots \mid 
			\delta_k\gamma$}\\
		\mbox{\hspace{0.5cm} where $A_j \rightarrow \delta_1 \mid 
			\cdots \mid 
			\delta_k$ are all the current $A_j$-productions}
		\end{FOR}\\
	\mbox{eliminate immediate left recursion among
			the $A_i$-productions}
	\end{FOR}	
	\end{algorithm}
	\end{minipage}

\w {\bf{}Left factoring} is a grammar transformation useful for
	producing a grammar suitable for {\sl\bfseries{}predictive
	parsing\/}.
	\bit
	\w Given two $A$-productions $A \rightarrow \alpha\beta_1 \mid
		\alpha\beta_2$
		\begin{eqnarray*}
		A & \rightarrow & \alpha{A'} \\
		A' & \rightarrow & \beta_1 \mid \beta_2
		\end{eqnarray*}
	\eit
\w {\bf{}Non-context-free language constructs}
	\bit
	\w $L_1 = \{wcw: \mbox{$w$ is in $(a|b)^*$}\}$ is {\em{}not \/}
		context-free.
	\w $L_2 = \{a^nb^mc^nd^m: n \ge 1, m \ge 1\}$ is {\em{}not\/}
		context-free.
	\w $L_3 = \{a^nb^nc^n: n \ge 0\}$ is {\em{}not\/} context-free.
	\w $L_1' = \{wcw^{\mbox{\scriptsize{}R}}: 
		\mbox{$w$ is in $(a|b)^*$}\}$ is context-free.
	\w $L_2' = \{a^nb^mc^md^n: n \ge 1, m \ge 1\}$ is 
		context-free.
	\w $L_3' = \{a^nb^n: n \ge 0\}$ is context-free.
	\eit
\eit


\paragraph{Top-Down Parsing: Recursive-Descent Parsing}
\bit
\w {\bf{}Predictive parsing} is a recursive-descent parsing that
	does not require backtracking; {\em{}backtracking\/} means 
	repeated scans of the input.
\w A left-recursive grammar can cause a recursive-descent parser, even
	one with backtracking, to go into an infinite loop.
\w {\em{}To construct a {\bfseries{}predictive parser\/}, we must know, given
	the current input symbol $a$ and the nonterminal
	$A$ to be expanded, which one of the alternatives of 
	production $A \rightarrow \alpha_1 \mid \cdots \mid
	\alpha_k$ is the {\bfseries{}unique} alternative that
	derives a string beginning with $a$\/}.
\w {\bf{}Transitive diagrams for predictive parsers}
	\bit
	\w There's one diagram for each nonterminal.
	\w The labels of edges are tokens and nonterminals.
	\w A transition on a token (terminal) means we should take
		that transition if that token is the next input symbol.
	\w A transition on a terminal $A$ is a {\em{}call of the
		procedure for $A$\/}.
	\eit
\w {\bf{}Nonrecursive predictive parsing}
	\bit
	\w We can build nonrecursive predictive parser using explicit stacks.

	\vspace{0.2cm}

	\begin{minipage}[b]{20cm}
	\scriptsize
	\begin{algorithm}{NonrecursivePredictiveParse}{M, w}
	\mbox{\tt{}// $M$ is a parsing table for grammar $G$}\\
	\mbox{\tt{}// and $w$ is a string}\\
	\mbox{\tt{}// This algorithm outputs leftmost derivation}\\
	\mbox{\tt{}// of $w$ if $w \in L(G)$}\\
	ip \= \mbox{index for the first symbol of $w$\$}\\
	\begin{REPEAT}{}
		X \= \mbox{top stack symbol}\\
		a \= w[ip]\\
		\begin{IF}{\mbox{$X$ is a terminal or \$}}
			\begin{IF}{X = a}
				\mbox{pop $X$ and advance $ip$}
			\ELSE
				\mbox{ERROR}
			\end{IF}
		\ELSE
			\begin{IF}{M[X, a] = X \rightarrow Y_1\cdots{Y_k}}
				\mbox{pop $X$}\\
				\mbox{push $Y_k, Y_k-1, \cdots, Y_1$ with
					$Y_1$ as top}\\
				\mbox{output $X \rightarrow Y_1\cdots{Y_k}$}
			\ELSE
				\mbox{ERROR}
			\end{IF}
		\end{IF}
	\end{REPEAT}{X = \$}
	\end{algorithm}
	\end{minipage}

	\eit
\w If $\alpha$ is any string of grammar symbols,
	{\bf{}FIRST}$(\alpha)$ is the set of terminals that begin the strings
	derived from $\alpha$.
	\bit
	\w If $\alpha \stackrel{+}{\Ra} \epsilon$, then $\epsilon$ is
		also in FIRST$(\alpha)$.
	\eit
\w For a {\em{}nonterminal\/} $A$, {\bf{}FOLLOW}$(A)$ is the set of terminals
	$a$ that can appear immediately to the right of $A$ in some
	sentential form.
	\bit
	\w FOLLOW$(A) = \{a : (\exists{\alpha,\beta})[S 
		\stackrel{\ast}{\Ra} \alpha{Aa}\beta]\}$.
	\w If $A$ can be the rightmost symbol in some sentential form,
		then \$ is in FOLLOW$(A)$.
	\eit
\w {\bf{}Computing FIRST$(X)$ for a grammar symbol $X$}
	\ben
	\w If $X$ is a terminal, then FIRST$(X) = \{X\}$.
	\w If $X \rightarrow \epsilon$ is a production,
		then add $\epsilon$ to FIRST$(X)$.
	\w If $X \rightarrow Y_1Y_2\cdots{Y_k}$ is a production, 
		then place $a$ in FIRST$(X)$ if for some $i$, $a$ is
		in FIRST$(Y_i)$, and $\epsilon$ is in all of
		FIRST$(Y_1)$, $\cdots$, FIRST$(Y_{k-1})$; i.e.,
		$Y_1\cdots{Y_{k-1}} \stackrel{\ast}{\Ra} \epsilon$.
		If $\epsilon \in \mbox{FIRST}(Y_j)$ for all
		$j = 1, 2, \cdots, k$, then add $\epsilon$ to FIRST$(X)$.
	\een
\w {\bf{}Computing FOLLOW$(A)$ for a nonterminals $A$}
	\ben
	\w Place \$ in FOLLOW$(S)$, where $S$ is the start symbol
		and \$ is the input right endmarker.
	\w If there is a production of the form 
		$A \rightarrow \alpha{B}\beta$,
		then everything in FIRST$(\beta)$ except for
		$\epsilon$ is placed in FOLLOW$(B)$.
	\w If there is a production $A \rightarrow \alpha{B}$, or a 
		production $A \rightarrow \alpha{B}\beta$ where
		FIRST$(\beta)$ contains $\epsilon$ (i.e.,
		$\beta \stackrel{\ast}{\Ra} \epsilon$), then everything
		in FOLLOW$(A)$ is in FOLLOW$(B)$.
	\een
\w {\bf{}Construction of predictive parsing tables}
	\bit
	\w If $A \rightarrow \alpha$ is a production with $a$ in 
		FIRST$(\alpha)$, then the parser will {\em{}expand $A$ by
		$\alpha$ when the current input symbol is $a$\/}.
	\w The only complication occurs when $\alpha = \epsilon$
		or $\alpha \stackrel{\ast}{\Ra} \epsilon$, in which case,
		we should again {\em{}expand $A$ by $\alpha$ if 
		the current 
		symbol is in FOLLOW$(A)$\/}, or if the \$ on the input
		has been reached and \$ is in FOLLOW$(A)$.
	\eit
\w A grammar whose parsing table has no multiply-defined entries is
	said to be {\bf{}LL(1)}.
	\bit
	\w First `L' stands for scanning the input from left to right.
	\w Second `L' stands for producing a {\em{}leftmost derivation\/}.
	\w `1' stands for using {\em{}one input symbol of lookahead\/}.
	\eit
\w {\bf{}Properties of LL(1)}
	\bit
	\w No ambiguous or left-recursive grammar can be LL(1).
	\w A grammar $G$ is LL(1) iff whenever $A \rightarrow \alpha | \beta$
		are two distinct productions of $G$ the following 
		conditions hold:
		\ben
		\w [(a)] For no terminal $a$ do both $\alpha$ and $\beta$
			derive strings beginning with $a$.
		\w [(b)] At most one of $\alpha$ and $\beta$ can derive
			the empty string.
		\w [(c)] If $\beta \stackrel{\ast}{\Ra} \epsilon$, then 
			$\alpha$ does not derive any string beginning with
			a terminal in FOLLOW$(A)$.
		\een
	\eit
\w Some grammars cannot be transformed into LL(1) just by eliminating
	left recursions and by left factorings. Currently there  is no
	universal rules for dealing with multiply-defined entries without
	affecting the language being recognized by the parser.
\eit

\paragraph{Bottom-Up Parsing}
\bit
\w {\bf{}Shift-reduce parsing} attempts to construct a parse tree for an
	input string beginning at the leaves and working up toward the
	root.
	\bit
	\w We can think of this process as one of {\em{}reducing\/}
		a string $w$ to the start symbol of a grammar.
	\w At each reduction step, a particular substring matching the
		right side of a production is replaced by the symbol
		on the left of the production.
	\w If the substring is chosen correctly at each step, a
		{\em{}rightmost\/} derivation is traced out in reverse.
	\eit
\w {\bf{}Handles}
	\bit
	\w A {\bf{}handle} of a right-sentential form $\gamma$ is a
		production $A \rightarrow \beta$ and a position of $\gamma$
		where the string $\beta$ may be found and replaced by $A$
		to produce the previous right-sentential form in a 
		rightmost derivation of $\gamma$
	\w If $S 
		\stackrel{\ast}{\Ra}_{\mbox{\scriptsize{}rm}} \alpha{A}w
		\stackrel{\ast}{\Ra}_{\mbox{\scriptsize{}rm}} \alpha\beta{w}$,
		then $A \rightarrow \beta$ in the position following
		$\alpha$ is a handle of $\alpha\beta{w}$.
	\w If a grammar is unambiguous then every right-sentential form
		of the grammar has exactly one handle.
	\w A handle of a string is a substring that matches the right side
		of a production, and whose reduction to the nonterminal
		on the left of the production represents one step
		along the reverse of a rightmost derivation.
	\eit
\w Example: in the following sequence of right derivations, the underlined
	strings are the handles.
	\begin{eqnarray*}
	E & \Ra_{\mbox{\scriptsize{}rm}} & \underline{E + E}\\
	 & \Ra_{\mbox{\scriptsize{}rm}} & E + \underline{E \ast E} \\
	 & \Ra_{\mbox{\scriptsize{}rm}} & E + E \ast 
					\underline{\mbox{\bf{}id}}\\
	 & \Ra_{\mbox{\scriptsize{}rm}} & E + \underline{\mbox{\bf{}id}} \ast
					\mbox{\bf{}id}\\
	 & \Ra_{\mbox{\scriptsize{}rm}} & \underline{\mbox{\bf{}id}} +
					\mbox{\bf{}id} \ast \mbox{\bf{}id}
	\end{eqnarray*}
\w A rightmost derivation in reverse can be obtained by 
	{\bf{}handle pruning}. 
	\bit 
	\w Start with a string of terminals $w$ that we wish to parse.
	\w Let $\gamma_n$ be the $n$th right-sentential form s.t.
		\[ S = \gamma_0\ \Ra_{\mbox{\scriptsize{}rm}}\ 
		\gamma_1\ \Ra_{\mbox{\scriptsize{}rm}}\ \cdots\ 
		\Ra_{\mbox{\scriptsize{}rm}}\ \gamma_n = w \]
	\w Locate the handle $\beta_n$ in $\gamma_n$ and replace $\beta_n$
		by the left side of some production $A_n \rightarrow \beta_n$
		to obtain $\gamma_{n-1}$, and go on.
	\eit
\w {\bf{}Stack implementation of shift-reduce parsing}
	\bit
	\w Two problems for handle-pruning-based parsing
		\ben
		\w [(a)] need to {\em{}locate the substring to be reduced\/}
			in a right-sentential form
		\w [(b)] need to determine {\em{}what production to choose\/}
			in case there are more than one productions with
			that substring on the right side.
		\een
	\w Data structures
		\bit
		\w {\em{}stack\/} to hold grammar symbols
		\w {\em{}input buffer\/} to hold string $w$ to be parsed
		\eit
	\w The parser operates by {\em{}shifting\/} zero or more input symbols 
		onto the stack until a handle $\beta$ is on the stack.
		The parser then {\em{}reduces\/} $\beta$ to the left
		side of the appropriate production.
	\w The handle will always eventually appear on the top of the stack,
		never inside.
	\eit
\w The set of prefixes of right sentential forms that can appear on the
	stack of a shift-reduce parser are called {\bf{}viable prefixes}.
	\bit
	\w A viable prefix can defined also as a prefix of right-sentential
		form that does not continue past the right end of the
		rightmost handle of that sentential form.
	\w It is always possible to add terminal symbols to the end of a 
		viable prefix to obtain a right-sentential form.
	\w Therefore, there is apparently no error as long as the portion of
		the input seen to a given point can be
		reduced to a viable prefix.
	\eit
\w {\bf{}Conflicts during shift-reduce parsing}
	\bit
	\w {\bf{}Shift/reduce conflict}: A parser cannot decide whether to
			shift or reduce.
	\w {\bf{}Reduce/reduce conflict}: A parser cannot decide which of 
			several reductions to make.
	\w Example:
		\begin{eqnarray*}
		stmt & \rightarrow & \mathbf{if\ } expr \mathbf{\ then\ } 				stmt\\
		& | & \mathbf{if\ } expr \mathbf{\ then\ } stmt 
			\mathbf{\ else\ } stmt\\
		& | & \mathbf{other}
		\end{eqnarray*}
		\bit
		\w shift/reduce conflict
			\bit
			\w Stack: $\cdots \mathbf{if\ } expr \mathbf{\ then\ }
				stmt$
			\w Input: $\mathbf{else\ } \cdots$\$
			\eit
		\eit
	\eit
\eit

\paragraph{Operator-Precedence Parsing}
\bit
\w A grammar where no production right side has two adjacent nonterminals
	is called an {\bf{}operator grammar}; 
	for the discussion of this paragraph we will add one more property
	that no production right side has $\epsilon$.
\w In operator-precedence parsing, we define three disjoint
	{\bf{}precedence relations}, $\rpre, \stackrel{\cdot}{=},
	\lpre$, between certain pairs of {\em{}terminals\/}.
\w The intention of the precedence relation is to delimit the handle
	of a right-sentential form, with $\rpre$ marking the left end,
	$\stackrel{\cdot}{=}$ in the interior of the handle,
	and $\lpre$ marking the right end.
	\bit
	\w Example: \$$\rpre \mathbf{\ id\ } \lpre + \rpre
		\mathbf{\ id\ } \lpre \ast \rpre
		\mathbf{\ id\ } \lpre$\$
	\eit
\w {\bf{}Deriving operator-precedence relations}
	\bit
	\w If operator $\theta_1$ has higher precedence thatn operator
		$\theta_2$, 
		\[ \theta_1 \lpre \theta_2, \quad \theta_2 \rpre \theta_1\]
	\w If $\theta_1$ and $\theta_2$ are operators of equal precedence,
		\bit
		\w if operators are left-associative,
		\[ \theta_1 \lpre \theta_2, \quad \theta_2 \lpre \theta_1 \]
		\w if operators are right-associative,
		\[ \theta_1 \rpre \theta_2, \quad \theta_2 \rpre \theta_1 \]
		\eit
	\w $\theta \rpre \mathbf{\ id}$,\  $\mathbf{id\ } \lpre \theta$, \ 
		$\theta \rpre ($, \ $( \rpre \theta$, \ 
		$) \lpre \theta$, \ $\theta \lpre )$, \ 
		$( \eqpre )$ \ $( \rpre ($, \ $) \lpre )$
	\eit
\w Compilers using oeprator-precedence parsers need not store the
	table of precedence relations. In most cases, the table can be
	encoded by two {\bf{}precedence functions} $f$ and $g$ that
	map terminal symbols to integers. We attempt to select
	$f$ and $g$ so that, for symbols $a$ and $b$,
	\ben
	\w [(a)] $f(a) < g(b)$ whenever $a \rpre b$,
	\w [(b)] $f(a) = g(b)$ whenever $a \eqpre b$,
	\w [(c)] $f(a) > g(b)$ whenever $a \lpre b$.
	\een
\eit
\paragraph{LR Parsers}
\bit
\w LR($k$) parsing method:
	\bit
	\w ``L'' is for left-to-right scanning, ``R'' for constructing
		a rightmost derivation in reverse, and $k$ for the number
		of input symbols of lookahead used for making 
		parsing decisions.
	\w LR parsing method is the most general {\em{}nonbacktracking\/}
		shift-reduce parsing method known, and easily implementable.
	\w Three techniques for cosntructing an LR parsing table for a grammar
		\bit
		\w {\bf{}Simple LR} (or {\bf{}SLR}): the easiest to implement
			but the least powerful of the three
		\w {\bf{}Canonical LR}: the most powerful but the hardest to
			implement
		\w {\bf{}Lookahead LR} (or {\bf{}LALR}): intermediate in 
			power and cost
		\eit
	\eit
\w {\bf{}The LR parsing algorithm}
	\bit
	\w An LR parser consists of an input, an output, a stack, a driver
		program, and a parsing table with `action' and `goto' parts.
		\bit
		\w A driver program is common to all LR parsers.
		\w The parsing table changes from one parser to another.
		\w Stack is used to store a string of the form
			\[ s_0X_1s_1X_2s_2 \cdots X_ms_m, \]
			where $s_m$ is on top. Each 
			$X_i$ is a grammar symbol
			and each $s_i$ is a symbol called a {\bf{}state}.
		\w Each state {\em{}summarizes information contained
			in the stack below\/}.
		\eit
	\eit
\w A {\bf{}configuration} of an LR parser is a pair whose first component
	is the stack contents and whose second component is
	the unexpended input:
	\[ (s_0 X_1 s_1 X_2 \cdots X_m s_m,\ a_i a_{i+1} \cdots a_n\mbox{\$})\]
	This configuration represents the {\em{}right-sentential form\/}
	\[ X_1 X_2 \cdots X_m a_i a_{i+1} \cdots a_n\mbox{\$}\]
\w {\bf{}Determinig the next move of the parser} when 
	the current input symbol is $a_i$ and the state on the top of
	the stack is $s_m$, where the current configuration is given by
	\[ (s_0 X_1 s_1 X_2 \cdots X_m s_m,\ a_i a_{i+1} \cdots 
		a_n\mbox{\$})\]
	\ben
	\w If $action[s_m, a_i] = $ \underline{shift $s$}, then the parser
		executes a shift move, entering the configuration
	\[ (s_0 X_1 s_1 X_2 \cdots X_m s_m a_i s,\ a_{i+1} \cdots 
		a_n\mbox{\$})\]
	\w If $action[s_m, a_i] = $ \underline{reduce $A \rightarrow \beta$},
		then the parser executes a reduce action, entering the
		configuration
	\[ (s_0 X_1 s_1 X_2 \cdots X_{m-r} s_{m-r} A s,\ a_i a_{i+1} \cdots 
		a_n\mbox{\$})\]
		where $s = goto[s_{m-r}, A]$ and $r = |\beta|$.
	\w If $action[s_m, a_i] = $ \underline{accept}, parsing is 
		completed.
	\w If $action[s_m, a_i] = $ \underline{error}, the parser calls
		an error recovery routine.
	\een
\w A grammar for which {\em{}we can construct a parsing table\/} as above
	is said to be an {\bf{}LR grammar}.
	\bit
	\w {\em{}There are context-free grammars that are not LR\/}, 
		but these
		can generally be avoided for typical programming languages.
	\w In order for a grammar to be LR it's sufficient that left-to-right
		shift-reduce parser be able to recognize handles
		when they appear on top of the stack.
	\eit
\w A $goto$ function defines essentially a {\bf{}finite state automaton}
	where each state summarizes the information on the grammar
	symbols in the stack; $goto[s_i, a] = s_j$ means 
	that when the current state is $s_i$ and grammar symbol $a$ is
	pushed, now the information incorporating $a$ is now $s_j$.
\w For a grammar to be LR$(k)$, we must be able to recognize the 
	occurrence of the right side of a production, having seen all of
	what is derived from that right side with $k$ input symbols of
	overhead.
	\bit
	\w This requirement is far less stringent than that for
		LL$(k)$ grammars where we must be able to recognize
		the use of a production seeing only the first $k$
		symbols of what its right side derives.
	\w LR grammars can describe more languages thatn LL grammar.
	\eit
\eit

\paragraph{Constructing SLR parsing tables}
\bit
\w An {\bf{}LR$(0)$ item} (or {\bf{}item}) of a grammar $G$ is
	a production of $G$ with a dot at some position of the
	right side.
\w Example: production $A \rightarrow XYZ$ yields four items, 
	$A \rightarrow \cdot{XYZ}$, $A \rightarrow X\cdot{YZ}$,
	$A \rightarrow XY\cdot{Z}$, and $A \rightarrow XYZ\cdot$.
\w An item can be represented by a pair of integers, one for
	the number of the production and the other for
	the position of the dot.
\w An item indicates {\em{}how much of a production we have
	seen at a given point in the parsing process\/}.
	$A \rightarrow X\cdot{YZ}$ means that we have just seen
	on the input a string derivable from $X$ and that we hope
	next to see a string derivable from $YZ$.
\w {\bf{}The central idea of SLR method}
	\bit
	\w The idea is to construct from the grammar a deterministic
		finite automaton to recognize viable prefixes.
	\w We can group items to form a state of the SLR parser.
	\w The items can be views as the states of an NFA ``recognizing''
		viable prefixes, and the ``grouping together'' is 
		essentially the subset construction process.
	\eit
\w One collection of sets of LR$(0)$ items, which we call {\bf{}canonical
	LR$(0)$} collection, provides the basis for constructing SLR parsers.
	\bit
	\w To construct the canonical LR$(0)$ collection for a grammar,
		we define an augmented grammar and two functions,
		$closure$ and $goto$.
	\eit
\w If $G$ is a grammar with start symbol $S$, then $G'$, the 
	{\bf{}augmented grammar} for $G$, is $G$ with a new start symbol
	$S'$ and production $S' \rightarrow S$. 
	\bit
	\w The purpose of this new starting production is to indicate
		to the parser when it should stop parsing and announce
		acceptance of the input. That is, acceptance occurs
		when and only when the parser is about to 
		reduce by $S' \rightarrow S$.
	\eit
\w {\bf{}The closure operation}
	\bit
	\w If $I$ is the set of items for a grammar $G$, then $closure(I)$
		is the set of items constructed from $I$ by the following
		rules:	
		\ben
		\w [(a)] Initially, every item in $I$ is added to 
			$closure(I)$.
		\w [(b)] If $A \rightarrow \alpha\cdot{B}\beta$ is in
			$closure(I)$ and $B \rightarrow \gamma$ is
			a production, then add the item
			$B \rightarrow \cdot\gamma$ to $I$, if it is
			not already there. We apply this rule until no
			more new items can be added to $closure(I)$.
		\een
	\w We can divide all the sets of items we are interested in into
		two classes of items.
		\bit
		\w {\bf{}Kernel items}, which include the initial items,
			$S' \rightarrow \cdot{S}$, and all items whose
			dots are not at the left end.
		\w {\bf{}Nonkernel items}, which have their dots at the
			left end.
		\eit
	\w We can save space if we throw away all nonkernel items, knowing
		that they could be regenerated by the closure process.
	\eit
\w {\bf{}The goto operation}
	\bit
	\w Intuitively, if $I$ is the set of items that are valid
		for some viable prefix $\gamma$, then $goto(I, X)$ is the
		set of items that are valid for the viable prefix 
		$\gamma{X}$.
	\w $goto(I, X)$ where $I$ is a set of items and $X$ is a grammar 
		symbol is 
		\[ goto(I, X) = closure(\{[A \rightarrow \alpha{X}\cdot\beta]:
			[A \rightarrow \alpha\cdot{X}\beta] \in I\}) \]
	\eit
\w {\bf{}The sets-of-items construction}

\begin{minipage}[t]{20cm}
{\scriptsize
\begin{algorithm}{Items}{G'}
C \= \{closure(\{[S' \rightarrow \cdot{S}]\})\}\\
\begin{REPEAT}{}
	\begin{FOR}{\mbox{each item-set $I \in C$ and each grammar
		$X$ s.t. $goto(I, X) \ne \emptyset,$ and $\not\in C$}}
		\mbox{add $goto(I, X)$ to $C$}
	\end{FOR}
\end{REPEAT}{\mbox{no more sets of items can be added to $C$}}
\end{algorithm}}
\end{minipage}

\w We say item $A \rightarrow \beta_1\cdot\beta_2$ is {\bf{}valid} for
	a viable prefix $\alpha\beta_1$ if there is a derivation
	$S' \stackrel{*}{\Rightarrow}_{\mbox{\scriptsize{}rm}}
	\alpha{A}w \stackrel{*}{\Rightarrow}_{\mbox{\scriptsize{}rm}}
	\alpha\beta_1\beta_2{w}$.
	\bit
	\w In general, an item will be valid for many viable prefixes.
	\w {\em{}The fact that $A \rightarrow \beta_1\cdot\beta_2$ is
		valid for $\alpha\beta_1$ tells us a lot about 
		whether to shift or reduce when we find $\alpha\beta_1$
		on the parsing stack.}
		\bit
		\w If $\beta_2 \ne \epsilon$ then it suggests that we have
			not yet shifted the handle onto the stack, so
			shift is our move.
		\w If $\beta_2 = \epsilon$ then it looks as if 
			$A \rightarrow \beta_1$ is the handle, and we should
			reduce by this production.
		\eit
	\eit
\w {\bf{}SLR parsing tables}: The 
	following procedure shows the algorithm for {\em{}constructing
	SLR parsign action and goto functions from the DFA that 
	recognizes the viable prefixes\/}.
	\ben
	\w // {\em{}Input: An augmented grammar $G'$}
	\w Construct $C = \{I_0, I_1, \cdots, I_n\}$, the collection of
		LR($0$) items for $G'$.
	\w State $i$ is constructed from $I_i$ and the parsing actions for
		state $i$ are determined as follows:
		\ben
		\w If $[A \rightarrow \alpha\cdot{a}\beta]$ is in $I_i$
			and $goto(I_i, a) = I_j$, then
			\[ action[i, a] \leftarrow \mbox{``shift $j$''}\]
		\w If $[A \rightarrow \alpha\cdot]$ is in $I_i$, then
			for all $a \in$ FOLLOW$(A)$
			\[ action[i, a] \leftarrow 
			\mbox{``reduce $A \rightarrow \alpha$''} \]
			here $A$ may not be $S'$.
		\w If $[S' \rightarrow S\cdot]$ is in $I_i$, then
			\[ action[i, \mbox{\ \$}] \leftarrow
			\mbox{``accept''}\]
		\een
	\w // {\em{}If any conflicting actions are generated by the above
		rules, the grammar is not in SLR(1)\/}.
	\w The $goto$ transitions for state $i$ are constructed for
		all terminals $A$ using the rule:
		if $goto(I_i, A) = I_j$, then
		$goto[i, A] = j$.
	\w All entries not defined by rules (3) and (4) are made
		``error.''
	\w The initial state of the parser is the one 
		constructed from the set of items containing
		$[S' \rightarrow \cdot{S}]$.
	\een

\eit




%%%%%%%%
\section{Syntax-Directed Translation}
\bit
\w There are two notations for associating semantics rules
	with productions. After building a parse tree, 
	we can traverse the tree as needed to evaluate the
	semantic rules at the parse-tree nodes.
	\bit
	\w {\em{}syntax-directed definitions\/}
	\w {\em{}translation schemes\/}: indicates the order in 
		which translation takes place
	\eit
\eit
\paragraph{Syntax-Directed Definitions}
\bit
\w A syntax-directed definition is a generalization of a context-free
	grammar in which each ``grammar symbol'' has an associated
	set of {\em{}attributes\/}, partitioned into two subsets
	called the {\em{}synthesized\/} and {\em{}inherited\/} attributes
	of that grammar symbol.
	\bit
	\w An {\bf{}attribute} can represent anything we choose: a string,
		a number, a type, a memory location, or whatever.
	\w The value of an attribute at a node is defined by {\em{}a
		semantic rule associated with the production used
		at the node\/}.
	\eit
\w A parse tree showing the values of attributes at each node
	is called an {\bf{}annotated parse tree}.
	\bit
	\w The process of computing the attribute values is called 
		{\em{}annotating\/} or {\em{}decorating\/} the
		parse tree. 
	\eit
\w In a syntax-directed definition, each grammar production $A \rightarrow
	\alpha$ has associated with it a set of semantic rules
	of the form $b := f(c_1, c_2, \cdots, c_k)$ where 
	$f$ is a function, and either
	\ben
	\w $b$ is a {\bf{}synthesized attribute} of $A$ and 
		$c_1, \cdots, c_k$ are attributes belonging to the grammar
		symbol of the production, or
	\w $b$ is an {\bf{}inherited attribute} of {\em{}one of 
		the grammar symbols on the right side of the production\/}
		and $c_1, \cdots, c_k$ are attributes belonging
		to the grammar symbols of the production.
	\een
\w We say that the attribute $b$ {\em{}depends on\/}
	attributes $c_1, \cdots, c_k$.
\w An {\bf{}attribute grammar} is a syntax-directed definitino in 
	which the functions in semantic rules cannot have side
	effects.
\w Values for attributes are usually supplied by the lexical analyzers, 
	e.g., in 
	\vspace{0.1cm}

	\centerline{\begin{tabular}{l|l}\hline
	Production & Semantic rules \\ \hline
	$F \rightarrow ( E )$ &  $F.val := E.val$\\ 
	$F \rightarrow \mbox{\bf{}digit}$ &  $F.val := 
		\mbox{\bf{}digit}.val$\\ \hline
	\end{tabular}}

\w {\bf{}Synthesized attributes}
	\bit
	\w A syntax-directed definition that used synthesized 
		attributes exclusively is said to be an 
		{\bf{}S-attributed definition}.
	\eit
\w {\bf{}Inherited attributes}
	\bit
	\w An {\em{}inherited attribute\/} is one whose value at
		a node in a parse tree is defined in terms of 
		attributes at the {\em{}parent\/} and/or 
		{\em{}siblings\/} of that node.
	\w Inherited attributes are convenient for expressing
		the dependence of a programming language construct
		on the `context' in which it appears.
		\bit
		\w (Example) Given an identifier, we can use the
			inherited attribute to keep trak of whether
			it appears on the left side or right side
			of the assignment operator, and decide
			whether l-value or r-value is needed.
		\eit
	\w So when inherited attributes are used, referential
		transparency is not converved.
	\w (Example) Determining types

	\vspace{0.2cm}

	\centerline{\begin{tabular}{l|l}\hline
	Production & Semantic rules \\ \hline
	$D \rightarrow T L$ & $L.in := T.type$ \\
	$T \rightarrow \mbox{\bf{}int}$ & $T.type = int$ \\
	$T \rightarrow \mbox{\bf{}real}$ & $T.type = real$ \\
	$L \rightarrow L_1, \mbox{\bf{}id}$ & $L_1.in := L.in$ \\
		& $addtype(\mbox{\bf{}id}.entry, L.in)$ \\
	$L \rightarrow \mbox{\bf{}id}$ & 
		$addtype(\mbox{\bf{}id}.entry, L.in)$ \\ \hline
	\end{tabular}}
	\eit
\w {\bf{}Dependency graphs}
	\bit
	\w Dependency graphs show shuch precendence relations on
		attributes.
	\w If an attribute $b$ depends on an attribute $c$, then
		the semantic rule for $c$ must be evaluated for
		that of $b$.
	\w In the above case a directed edge 
		$b \rightarrow c$ is added to the graph.
	\eit
\w {\bf{}Evaluation order}
	\bit
	\w A {\bf{}topological sort} of a DAG is any ordering
		of the nodes ofthe graph that preserves each 
		ordering between two nodes.
	\eit
\w Methods for evaluating semantic rules
	\ben
	\w [(a)] {\em{}Parse-tree methods\/}: 
		uses topological sort to obtain a proper evaluation order;
		fails if the dependency graph is not a DAG, i.e, there a
		directed cycle in the graph
	\w [(b)] {\em{}Rule-based methods\/}:
		for each production, the order in which the
		production is evaluated is predetermined at
		{\em{}compiler-construction time\/} using 
		rule-based method
	\w [(c)] {\em{}Oblivious methods\/}:
		evaluation order is chosen without considering
		the semantic rules; the order is forced by the parsing
		method, independent of the semantic rules
	\een
\w Since rule-based methods and oblivious methods does not construct
	dependency graphs, they may be more efficient.
\eit
\paragraph{Construction of Abstract Syntax Trees}
\bit
\w {\em{}The use of abstract syntax trees as an intermidiate representation
	allows translation to be decoupled from parsing\/}.
\w A {\bf{}$($abstract$)$ syntax tree} is a condensed form of 
	parse tree useful for representing {\em{}language constructs\/}.
	\bit
	\w (Example) $S \rightarrow $ {\bf{}if $B$ then $S_1$ else $S_2$}
	\[ \xymatrix@-1.0pc{
		& \mbox{\bf{}if-then-else} \ar@{-}[dl] \ar@{-}[d] 
		\ar@{-}[dr] & \\
	B & S_1 & S_2
	}
	\]
	\eit
\w In a syntax tree, operators and keywords do not appear as leaves,
	but rather are associated with the interior node that would
	be the parent of those leaves in the parse tree.
\w {\bf{}Constructing syntax trees for expressions}
	\bit
	\w Basic operations
		\bit
		\w $mknode(op, left, right)$
		\w $mkleaf(\mbox{\bf{}id}, entry)$
		\w $mkleaf(\mbox{\bf{}num}, val)$
		\eit
	\eit
\w {\bf{}Syntax-directed definition for constructing syntax trees}
	\bit
	\w We do not evaluate the programs directly but 
		`evaluate' them to the `intermediate representation,'
		the abstract syntax trees.
	
	\vspace{0.2cm}
	
	\centerline{\begin{tabular}{l|l}\hline
	Production & Semantic rules \\ \hline
	$E \rightarrow E_1 + T$ &
		$E.nptr := mknode('+', E_1.nptr, T.nptr)$ \\
	$E \rightarrow E_1 - T$ &
		$E.nptr := mknode('-', E_1.nptr, T.nptr)$ \\
	$E \rightarrow T$ &
		$E.nptr := T.nptr$\\
	$T \rightarrow ( E )$ &
		$T.nptr := E.nptr$ \\
	$T \rightarrow \mbox{\ \bf{}id}$ &
		$T.nptr := mkleaf(\mbox{\bf{}id}, 
			\mbox{\ \bf{}id}.entry)$ \\
	$T \rightarrow \mbox{\ \bf{}num}$ &
		$T.nptr := mkleaf(\mbox{\bf{}id}, 
			\mbox{\ \bf{}num}.val)$ \\ \hline
	\end{tabular}}
	\eit
\w {\bf{}Directed acyclic graphs for expressions}
	\bit
	\w In DAGs, common subexpressions of an expression is 
		represented as a single node instead of multiple
		identical nodes.
	\w A DAG is obtained if the function constructing a node
		first checks to see whether an identical node 
		already exists.
	\w Each node is represented as an entry of a record-array.
	\w The integer index of a node is called a {\bf{}value number}.
	\eit
\eit

\paragraph{Bottom-up Evaluation of S-Attributed Definitions}
\bit
\w Synthesized attributes can be evaluated by a {\em{}bottom-up 
	parser\/} as the input is being parsed.
	\bit
	\w The parser can keep the values of the synthesized
		attributes associated with the grammar symbols
		on its stack.
	\eit
\eit

\section{Run-time Environments}

\section{Intermediate Code Generation}


\bibliographystyle{plain}
\bibliography{bib/mac,bib/pl,bib/theory}
\nocite{AU72}
\tableofcontents
\end{document}

% LocalWords:  lexeme al Kleene Nonregular NFA iff DFA SubsetConst Dstates EOF
% LocalWords:  Dtran SimulateNFA nextchar grep lexemes RegEx NFA's nullable pc
% LocalWords:  firstpos lastpos followpos ConstructDFA  nonaccepting Cocke LL
% LocalWords:  Kanellakis Smolka DFA's LocalWords Kasami Earley's LR ip XYZ SLR
% LocalWords:  EliminateLeftRecursion tokenization Nonrecursive nonrecursive
% LocalWords:  NonrecursivePredictiveParse  endmarker Nonkernel nonkernel
% LocalWords:  losure
