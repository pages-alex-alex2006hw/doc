\documentclass{article}
%\usepackage{theorem,mydef,amssymb,newalg}
\usepackage{hfont,theorem,mydef,amssymb,newalg}
\begin{document}

\title{\Large\bf{}Notes on Algorithms}
\author{¡§ √∂¡÷}
\maketitle

%heapsort
\section{Heapsort}
\subsection{Summary}
\bit
\w Heapsort sorts {\em{}in place\/}.
\w Heapsort is not {\em{}stable\/}.
\w Heapsort works in $O(n\lg n)$ time, which is optimal.
\eit


\subsection{Heaps}
\bit
\w {\bf{}Heap} data structure
	\bit
	\w Heaps are represented by a {\em{}complete binary tree}.
	\w {\bf{}Heap property}: $A[\mbox{\sf{}Parent}(i)] \ge A[i]$
	\w The {\bf{}height} of a node $v$ is the 
		{\em{}number of edges on the longest simple path
		from $v$ to a leaf\/}.
		\bit
		\w The {\bf{}height} of a tree is the height of its root.
		\eit
	\w The {\bf{}depth} of a node $v$ is the {\em{}number of
		edges on the simple path from $v$ to the root\/}.
		\bit
		\w The depth of the root is $0$ and the maximum depth of 
			nodes is equal to the height of the tree.
		\eit
	\w In a heap of height $h$, 
		the number of nodes, $N_t$, at depth $t$ in a heap satisfies:
		\bit
		\w $N_t = 2^{t}$ if $t < h$.
		\w $1 \le N_t = N_h \le 2^h$ if $t = h$. 
		\eit
	\w The number of elements, $n$, of a heap of height $h$ satisfies
		\[ 2^h \le n < 2^{h+1}.  \]
		since, 
		\begin{eqnarray*}
		n & = & \sum_{t = 0}^hN_t\\
		& = & N_0 + N_1 + \cdots + N_h\\
		& = & 2^0 + 2^1 + \cdots 2^{h-1} + N_h\\
		& = & 2^h - 1 + N_h
		\end{eqnarray*}
		and 
		\begin{eqnarray*}
		2^h - 1 + 1 \le & 2^h - 1 + N_0  = n & \le 2^h - 1 + 2^h\\
		2^h \le & n & \le 2^{h+1} - 1
		\end{eqnarray*}
	\w An $n$-element heap has height $\floor{\lg n}$, since
		\begin{eqnarray*}
		2^h \le & n & < 2^{h+1}\\
		h \le & \lg n & < h+1
		\end{eqnarray*}
		and $h = \floor{\lg n}$ by definition of $\floor{\cdot}$.
	\eit
\eit
\subsection{Maintaining the Heap Property}
\bit
\w \fbox{{\sf{}Heapify}$(A, i)$}
	\bit
	\w {\bf{}Precondition}: given that subtrees 
		rooted at {\sf{}Right}$(i)$ and
		{\sf{}Left}$(i)$ are heaps, but $A[i]$ may be smaller than
		its children, thus violating the heap property
	\w {\bf{}Postcondition}: subtree rooted at $i$ is a heap
	\w {\bf{}Time complexity}: \fbox{$O(h) \equiv O(\lg n)$}, 
		where $h$ is the height of $i$ 
		\bit
		\w Children's subtrees each have size at most $2n/3$ --
			the worst case occurs when the {\em{}last row of the
			tree is exactly half full\/}\footnote{When
			the ratio $\frac{\mbox{order of the larger 
			subtree}}{\mbox{order
			of the smaller subtree}}$ is maximized.}:
			\begin{quote}
			Let $h$ be the height of the tree rooted at $i$.
			Then, 
			\begin{eqnarray*}
			n & = & (2^h - 1) + (2^{h-1} -1) + 1\\
			& = & 2^h + 2^{h-1} - 1.
			\end{eqnarray*}
			The order of the left subtree is $n' = 2^h - 1$ and
			\begin{eqnarray*}
			\frac{n'}{n} & = & \frac{2^h - 1}{2^h + 2^{h-1} - 1}\\
			& = & \frac{2\cdot{2^{h-1}} - 1}{3\cdot{2^{h-1}} - 1}\\
			& < &\frac{2\cdot{2^{h-1}}}{3\cdot{2^{h-1}}} 
			= \frac{2}{3}.
			\end{eqnarray*}
			\end{quote}
		\w $T(n) \le T(2n/3) + \Theta(1)$
		 	\  $\Rightarrow$\  $T(n) = O(\lg n)$.
		\eit
	\eit
\eit
\subsection{Building a Heap}
\bit
\w How can we convert $A[1..n]$ into a heap, where 
	$n = \mbox{\em{}\ length}(A)$?
\w \mbox{{\sf{}Build-Heap}$(A)$}
	\bit
	\w {\sf{}Build-Heap}
		uses $\floor{\mbox{\em{}length}(A)/2}$ {\sf{}Heapify}s
		in a {\em{}bottom-up manner\/}.
		\bit
		\w note that $A[\floor{n/2}] \cdots A[n]$ are leaves!
		\w At first sight, this algorithms seems to work with
			$O(n)$ calls of $O(\lg n)$-time {\sf{}Heapify}'s:
			hence with $O(n\lg n)$-complexity.
		\w (cont.) But, we can get a tighter bound.
		\eit
	\w An $n$-element heap has at most $\ceil{n/2^{h+1}}$ nodes of 
		height $h$.
		\bit
		\w Why? Prove by mathematical induction. That's the
			only proof currently known to me.
		\eit
	\w {\bf{}Time Complexity}: \fbox{$O(n)$}
		\[ \sum_{h=0}^{\floor{\lg n}} \ceil{\frac{n}{2^{h+1}}} O(h)
			= O\left(n\sum_{h=0}^{\floor{{\lg n}}}\frac{h}{2^h}\right)
			= O(n).\]
		Note that $\sum_{k=0}^\infty{}kx^k = 
			\frac{x}{(1-x)^2}$ is an OGF of $\{k\}_{k\ge0}$.
	\w {\sf{}Build-Heap} can be implemented using {\bf{}sift-up}
		procedures but this results in $\Omega(n\lg n)$-time.
	\eit
\w \fbox{{\sf{}Heapsort}$(A)$}
	\bit
	\w Uses {\sf{}Build-Heap} and {\sf{}Heapify}'s.
	\w {\bf{}Time Complexity}: \fbox{$O(n\lg n)$}
	\eit
\eit

\subsection{Priority Queues}
\bit
\w {\bf{}Priority queue} ADT
	\bit
	\w {\sf{}Insert}$(S, x)$
	\w {\sf{}Maximum}$(S)$
	\w {\sf{}Extract-Max}$(S)$
	\eit
\w Priority queues can be implemented using heaps.
\w {\sf{}Insert} uses the technique of 
	{\bf{}sift-up} of an appended element.
\eit

%\subsection{$d$-ary Heaps}

%quicksort
\section{Quicksort}
\subsection{Performance}
\bit
\w {\bf{}Worst-case partitioning} 
	\bit 
	\w {\bf{}Time Complexity}: $\Theta(n^2)$
	\[ T(n) = T(n-1) + \Theta(n) = \Theta(n^2) \]
	\w $\Theta(n^2)$ even when the array is already sorted!
	\eit
\w {\bf{}Best-case partitioning} 
	\bit
	\w {\bf{}Time Complexity}: $\Theta(n\lg n)$
	\[ T(n) = 2T(n/2) + \Theta(n) = \Theta(n\lg n)\]
	\eit
\w {\bf{}Balanced partitioning}: $1:9$ partitioning
	\bit
	\w {\bf{}Time Complexity}: $\Theta(n\lg n)$
	\[ T(n) = T(n/9) + T(n/10) + \Theta(n) = \Theta(n\lg n)\]
	\eit
\eit
\subsection{Randomized Quicksort}
\bit
\w {\bf{}Advantages of randomization}
	\bit
	\w {\bf{}Spoiling the adversaries}:
		{\em{}no particular input elicits its worst-case behavior}!
	\eit
\eit

%bst
\section{Binary Search Trees}
\subsection{Summary}
\bit
\w Operations on binary search trees:
	{\sf{}Search},
	{\sf{}Minimum},
	{\sf{}Maximum},
	{\sf{}Predecessor},
	{\sf{}Successor},
	{\sf{}Insert},
	{\sf{}Delete}
\w The operations on a binary search tree take time proportional
	to the {\em{}height\/} of the tree.
	\bit
	\w So, if a binary search tree is not well-balanced, 
		the complexity of operations may be linear in the
		number of nodes, in the worst case.
	\eit
\eit

\subsection{What is a binary search tree?}
\bit
\w The keys in a binary search tree are always stored in such a way
	to satisfy the {\bf{}binary-search-tree property}:
	\begin{quote}
	Let $x$ be a node in a binary search tree.
	If $y$ is a node in the left subtree of $x$, then
	$key[y] \le key[x]$. If $y$ is a node in the right subtree
	of $x$, then $key[x] \le key[y]$.
	\end{quote}
\w The binary-search-tree property allows us to print out
	all the keys in a binary search tree in sorted order
	by a simple recursive algorithm, called the
	{\bf{}inorder tree walk}.
\w 
\eit


\bibliographystyle{plain}
\bibliography{00bib/mac,00bib/algo}
\nocite{CLR90}
\pagebreak
\tableofcontents
\end{document}
