\documentclass{myproc}
\usepackage{mydef,amssymb}
\begin{document}
\title{\large\bf Notes on Combinatorial Optimization \vspace*{-0.5cm}}
\author{\normalsize{}Cheoljoo Jeong $\arc{\mbox{cjeong@cs.columbia.edu}}$}
\maketitle
\small
\small
\section{Optimization Problems}
\subsection{Classification of problems}
\bit
\w \bb{Nonlinear programming problem}: Find $x$ to
	\begin{eqnarray*}
	\mbox{minimize} & f(x) & \\
	\mbox{subject to} & g_i(x) & \le 0, \quad i = 1, \cdots, m \\
	& h_j(x) & = 0, \quad j = 1, \cdots, p
	\end{eqnarray*}
where $f, g_i, h_j$ are general functions of parameter $x \in \R^n$.
\w \bb{Convex programming problems} 
are special cases of nonlinear programming problem, where
 $f$ is convex, $g_i$ is concave, and $h_j$ is linear.
In convex programming problems, local optimality implies global optimality
and there exist \bb{Kuhn-Tucker conditions}, sufficient conditions for
optimality of the solution.

\w \bb{Linear programming problems} are special cases of convex programming
problems, where $f, g_i, h_j$ are linear.
We only need to check finite set of possible candidate solutions, i.e.
the set of
vertices of the convex polytope defined by the linear constraints.

\w \bb{Integer linear programming problems} are those whose
solutions have integer-coordinates only.
\w Approaches to NP-complete problems
	\ben
	\w [(a)] approximation
	\w [(b)] enumerative techniques
	\w [(c)] local search
	\een
\eit
\subsection{Optimization problems}
\bit
\w An \bb{instance of an optimization problem} is a pair $(F, c)$ where
$F$ is any set, the domain of feasible points; $c$ is the cost
function, a mapping 
	\[ c: F \rightarrow \R^1. \]
The problem is to find an $f \in F$ for which
	\[ c(f) \le c(y) \mbox{\ for all\ } y \in F. \]
\w An \bb{optimization problem} is a set $I$ of instances of an
optimization problem. 
\w Given an optimization problem with instances $(F, c)$, a
\bb{neighborhood} is a mapping 
	\[ N: F \rightarrow 2^F \]
defined for each instance.
	\bit
	\w In LP, we can define a neighborhood by
	\[ N_\epsilon(x) = \{y: Ay = b, y \ge 0, \mbox{and\ } |y - x|
\le \epsilon\}.\] 
	\eit
\eit
\subsection{Local and global optima}
\bit
\w Given an instance $(F, c)$ of an optmization problem and a
neighborhood $N$, a feasible solution $f \in F$ is called \bb{locally
optimal with respect to $N$} if  
	\[ c(f) \le c(g), \mbox{\ for all\ } g \in N(f). \]
\w Given an optimization problem with feasible set $F$ and a
neighborhood $N$, if whenever $f \in F$ is locally optimal with
respect to $N$ it is also globally optimal, we say the neighborhood
$N$ is \bb{exact}. 
\eit
\subsection{Convex sets and functions}
\bit
\w Given two points $x, y \in \R^n$, a \bb{convex combination} of them
is any point of the form 
	\[ z = \lambda{x} + (1-\lambda)y,\quad \lambda \in \R^1 \mbox{\ and\ }
	0 \le \lambda \le 1.\]
	If $\lambda \ne 0, 1$, we say $z$ is a \bb{strict} convex
combination of $x$ and $y$. 
\w A set $S \subseteq \R^n$ is \bb{convex} if it contains all convex
combinations of pairs of points $x, y \in S$. 
\w {\sbf Lemma}: 
  The intersection of any number of convex sets $S_i$ is convex.
\w Let $S \subseteq \R^n$ be a convex set. The (cost) function
	\[ c: S \rightarrow \R^1 \]
is \bb{convex in $S$} if, for any two points $x, y \in S$,
\[ c(\lambda{x} + (1-\lambda)y) \le \lambda c(x) + (1-\lambda)c(y),\]
where $\lambda \in \R^1 \mbox{\ and\ } 0 \le \lambda \le 1$.
If $S = \R^n$, we say simply that $c$ is convex.
	\w Any {\em linear\/} function is convex in any convex set $S$.
	\w Intuitively, a convex function is one that ``bends up.''

\w {\sbf Lemma}: 
 Let $c(x)$ be a convex function on a convex set $S$. Then for any
real number $t$, the set  
\[S_t = \{x : c(x) \le t, \quad x \in S\}\]
is convex.
\w A function $c$ defined on a convex set $S \subseteq \R^n$ is called
\bb{concave} if $-c$ is convex in $S$.
\eit
\subsection{Convex programming problems}
\bit
\w An important class of optimization problems concerns the {\em
minimization of convex functions on a convex set\/}.
These problems have a convenient property that local optima
are global.

\w {\sbf Theorem}: 
Consider an instance of an optimization problem $(F, c)$, where
$F \subseteq \R^n$ is a convex set and $c$ is a convex function.
Then the neighborhood defined by Euclidean distance
\[ N_\epsilon(x) = \{y: y \in F \mbox{\ and\ } |x - y| \le
\epsilon\}\]
is exact for every $\epsilon > 0$.
\w An instance of an optimization problem $(F, c)$ is a \bb{convex
programming problem} if $c$ is convex and $F \subseteq \R^n$ is
defined by
\[ g_i(x) \ge 0, \quad i = 1, \cdots, m\]
where
\[ g_i:\R^n \rightarrow \R^1\]
are concave functions.
\w {\sbf Lemma}:
 The feasible set $F$ in a convex programming problem is convex.
\w In a convex programming problem, every point locally optimal with
respect to the Euclidean distance neighborhood $N_\epsilon$ is also
globally optimal.
\eit

\section{Linear Programming}
\subsection{Forms of linear programs}
\bit
\w Given an $m \times n$ matrix $A$ with rows $a_i'$, let $M$ be the set of
row indices corresponding to the equality constraints, and let $\overline{M}$ be
those of inequality constraints. 
And let $x \in \R^n$ and let $N$ be the column indices corresponding to
constraint variables and $\overline{N}$ be those of unconstrained variables.
Then an instance of the \bb{general linear program} is defined by
	\begin{eqnarray*}
	\mbox{minimize} & c^Tx & \\
	\mbox{subject to} & {a_i}^Tx = b_i & i \in M \\
	 & {a_i}^Tx \ge b_i & i \in \overline{M} \\
	& x_j \ge 0 & j \in {N} \\
	 & x_j \lesseqgtr 0 & j \in \overline{N}
	\end{eqnarray*}
where $b$ is an $m$-vector of integers and and $c$ an $n$-vector of integers.

\w The following is a linear program in \bb{canonical form}:
	\begin{eqnarray*}
	\mbox{minimize} & c^Tx & \\
	\mbox{subject to} & Ax \ge b &\\
	& x \ge 0 &
	\end{eqnarray*}
\w The following is a linear program in \bb{standard form}:
	\begin{eqnarray*}
	\mbox{minimize} & c^Tx & \\
	\mbox{subject to} & Ax = b &\\
	& x \ge 0 &
	\end{eqnarray*}
\w Pictorially, a standard form LP is minimizing
\[
	\left[\begin{array}{c}c_1\\c_2\\ \vdots \\ c_n	\end{array}\right] 
                \left[x_1\ x_2\ \cdots\ x_n\right]
\]
subject to
\[
	\left[\begin{array}{llll}
	    a_{11} & a_{12} & \cdots & a_{1n}\\
	    a_{21} & a_{22} & \cdots & a_{2n}\\
            \vdots & \vdots & \ddots & \vdots\\
	    a_{m1} & a_{m2} & \cdots & a_{mn}
	      \end{array}\right]
	\left[\begin{array}{c}x_1\\x_2\\ \vdots \\ x_n	\end{array}\right] 
       = 
	\left[\begin{array}{c}b_1\\b_2\\ \vdots \\ b_m	\end{array}\right] 
 \]
where $x_i \ge 0$, for $1 \le i \le n$.
\eit
\subsection{LP duality}
\bit
\w {\sbf Theorem}: If an LP has an optimal solution, so does its dual, and at
optimality their costs are equal.
\w {\sbf Theorem}: The dual of the dual is primal.
\w {\sbf Theorem}: Given a primal-dual pair, exactly one of three situations
 occurs, as indicated in the following table:\vspace*{0.2cm}\\
  \begin{tabular}{|c|c|c|c|}\hline
    primal$\setminus$dual & finite opt & unbounded & infeasible \\ \hline
   finite opt & $\bigcirc$ & $\times$ &$\times$\\
   unbounded & $\times$ & $\times$ & $\bigcirc$\\
   infeasible & $\times$ & $\bigcirc$ & $\bigcirc$ \\ \hline
 	     \end{tabular}
\eit

\section{Approximation Algorithms}
\subsection{LP-rounding}
\subsection{Primal-dual algorithms}
\subsection{Derandomization}
\section{Branch-and-Bound Algorithms}


\section{Local Search Algorithms}
\bit
\w To apply this approach to a particular problem, we should make 
a number of \bb{choices}:
  \bit
  \w how to obtain an \bb{initial feasible solution}
    (how many starting points to try)
  \w choice of a ``good'' neighborhood for the problem
  \eit
\w Large neighborhood potentially provides better local optima but it takes
  longer to find one.
\eit

\end{document}




% LocalWords:  Combinatorial optimality polytope combinatorial Enumerative Ay
% LocalWords:  optmization minimization Euclidean

