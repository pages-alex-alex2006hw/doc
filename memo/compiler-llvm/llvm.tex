\documentclass{myproc}
\usepackage{mydef,myenv}
%\usepackage{MinionPro}
\usepackage{mathptm}
\begin{document}
\small
%\Large
\title{LLC: Walkthrough}
%\author{\normalsize{}cjeong@live.com}
\date{}
\maketitle


\tableofcontents
%\pagebreak

\section{LLVM Basics}
\subsection{LLVM modules}
\bit
\w LLVM \bb{program} are composed of \bb{modules}, each of which is a
translation unit of the input programs. Each module consists of
\bb{functions}, \bb{global varaibles}, and \bb{symbol table entries}. 
\w Modules may be combined together with LLVM linker, which merges function
and global variable definitions, resolve forward declarations, and merges
symbol table entries. 
\eit
\subsection{Linkage types}
\bit
\w All \bb{global variables} and \bb{fuynctions} have a linkage type.
\eit
\subsection{Calling conventions}
\bit
\w LLVM \bb{functions}, \bb{calls} and \bb{invokes} can all have an optional
calling convention specified for the call.
\eit

\subsection{Global variables}
\bit
\w Global variables define {\em regions of memory\/} allocated at compilation
time instead of run-time. 
\w Can be defined as \bb{thread\_local}, meaning that it won't be shared by
threads. i.e. each thread will have a separate copy of the variable.
\eit

\subsection{Functions}
\bit
\w LLVM \bb{function definitions} consists of:
  \bit
  \w function name 
  \w (possibly empty) argument list (each with optional parameter attribute)
  \w list of basic blocks
  \eit
\eit

\subsection{Data Layout}
\bit
\w A module may specify a target specific data layout string that specifies
how data is laid out in memory.
\eit

\section{LLVM target-independent code generator}
The LLVM target-independent code generator is designed to support efficient
and quality code generation for standard register-based microprocessors. Code
generation in this model is divided into the following stages.

\subsection{Instruction selection}  This phase determines an efficient way to express the input LLVM code in the target instruction set. This stage produces the initial code for the program in the target instruction set, then makes use of virtual registers in SSA form and physical registers that represent any required register assignments due to target constraints or calling conventions. This step turns the \bb{LLVM code} into a \bb{DAG of target instructions}.

\subsection{Scheduling and formation} This phase takes the DAG of target
instructions produced by the instruction selection phase, determines an
\bb{ordering of the instructions}, then emits the instructions as
\verb+MachineInstr+s with that ordering. Note that we describe this in the
instruction selection section because it operates on a \bb{SelectionDAG}.

\subsection{SSA-based machine code optimizations} This optional stage consists
of a series of machine-code optimizations that operate on the SSA-form
produced by the instruction selector. Optimizations like modulo-scheduling or
peephole optimization work here.

\subsection{Register allocation} The target code is transformed from an {\em
  infinite virtual register file in SSA form\/} to the {\em concrete register
  file\/} used by the target. This phase introduces \bb{spill code} and
eliminates all virtual register references from the program.

\subsection{Prolog/epilog code insertion} Once the machine code has been
generated for the function and the amount of stack space required is known
(used for LLVM alloca’s and spill slots), the prolog and epilog code for the
function can be inserted and “abstract stack location references” can be
eliminated. This stage is responsible for implementing optimizations like
frame-pointer elimination and stack packing.

\subsection{Late machine code optimizations} Optimizations that operate on
``final'' machine code can go here, such as spill code scheduling and peephole
optimizations.

\subsection{Code emission} The final stage actually puts out the code for the
current function, either in the target assembler format or in machine code.

\section{LLVM target description classes}
The LLVM target, described by six different classes (located in
\verb+include/llvm/Target+), provides an abstract description of the target
machine. These classes are designed to represent abstract target properties
such as the instructions, registers it has, etc. 

\subsection{TargetMachine class}
The \bb{TargetMachine} class provides virtual methods used to acccess the
target-specific implementations of the various target description classes via
the \verb+get*Info+ methods (\verb+getInstrInfo+, \verb+getRegisterInfo+,
\verb+getFrameInfo+, etc.). This class is to be specialized by a concrete
target implementatino (e.g. \verb+X86TargetMachine+). 

\subsection{DataLayout class}
The \bb{DataLayout} class is the only required target description class, and
it is the only class that is not extensible (you cannot derive a new class
from it). DataLayout specifies information about how the target lays out
memory for structures, the alignment requirements for various data types, the
size of pointers in the target, and whether the target is little-endian or
big-endian. 

\subsection{TargetLowering class}
The \bb{TargetLowering} class is used by \bb{SelectionDAG}-based instruction
selectors primarily to describe how LLVM code should be lowered to
SelectionDAG operations. Among other things, this class indicates: 
 \bit
 \w an initial register class to use for various \bb{ValueType}s,
 \w which operations are natively supported by the target machine,
 \w the return type of \bb{setcc} operations,
 \w the type to use for shift amounts, and
 \w various high-level characteristics, like whether it is profitable to turn
 division by a constant into a multiplication sequence.
 \eit

\subsection{TargetRegisterInfo class}
The \bb{TargetRegisterInfo} class is used to describe the register file of the
target and any interactions between the registers. 

Registers are represented in the code generator by unsigned integers. Physical registers (those that actually exist in the target description) are unique small numbers, and virtual registers are generally large. Note that register \#0 is reserved as a flag value.

Each register in the processor description has an associated
\bb{TargetRegisterDesc} entry, which provides a textual name for the register
(used for assembly output and debugging dumps) and a set of aliases (used to
indicate whether one register overlaps with another). 

In addition to the per-register description, the \bb{TargetRegisterInfo} class
exposes a set of processor specific register classes (instances of the
TargetRegisterClass class). Each register class contains sets of registers
that have the same properties (for example, they are all 32-bit integer
registers). Each SSA virtual register created by the instruction selector has
an associated register class. When the register allocator runs, it replaces
virtual registers with a physical register in the set. 

The target-specific implementations of these classes is auto-generated from a \bb{TableGen} description of the register file.

\subsection{TargetInstrInfo class}
The \bb{TargetInstrInfo} class is used to describe the machine instructions
supported by the target. It is essentially an array of TargetInstrDescriptor
objects, each of which describes one instruction the target
supports. Descriptors define things like the mnemonic for the opcode, the
number of operands, the list of implicit register uses and defs, whether the
instruction has certain target-independent properties (accesses memory, is
commutable, etc), and holds any target-specific flags. 


\subsection{TargetFrameInfo class}
The \bb{TargetFrameInfo} class is used to provide information about the
\bb{stack frame layout} of the target. It holds the direction of stack growth,
the known stack alignment on entry to each function, and the offset to the
local area. The offset to the local area is the offset from the stack pointer
on function entry to the first location where function data (local variables, 
spill locations) can be stored. 

\subsection{TargetSubtarget class}
The \bb{TargetSubtarget} class is used to provide information about the
specific chip set being targeted. A sub-target informs code generation of
which instructions are supported, instruction latencies and instruction
execution itinerary; i.e., which processing units are used, in what order, and
for how long. 


\subsection{TargetJITInfo class}
The \bb{TargetJITInfo class} exposes an abstract interface used by the
Just-In-Time code generator to perform target-specific activities, such as
emitting stubs. If a TargetMachine supports JIT code generation, it should
provide one of these objects through the getJITInfo method.

\section{LLVM machine code description classes}
At the high-level, LLVM code is translated to a machine-specific
representation formed out of \bb{MachineFunction}, \bb{MachineBasicBlock}, and 
\bb{MachineInstr} instances (in \verb+include/llvm/CodeGen+). This
representation 
is completely target agnostic, representing instructions in their most
abstract form: an \bb{opcode} and a series of \bb{operands}. This
representation is designed to support both an SSA representation for machine
code, as well as a register allocated, non-SSA form. 

\subsection{MachineInstr class}
\subsection{MachineBasicBlock class}
The MachineBasicBlock class contains a list of machine instructions ( MachineInstr instances). It roughly corresponds to the LLVM code input to the instruction selector, but there can be a one-to-many mapping (i.e. one LLVM basic block can map to multiple machine basic blocks). The MachineBasicBlock class has a “getBasicBlock” method, which returns the LLVM basic block that it comes from.
The MachineFunction class

\subsection{MachineFunction class}
The \bb{MachineFunction} contains a list of machine basic blocks
(\bb{MachineBasicBlock} instances). It corresponds one-to-one with the LLVM
function input to the instruction selector. In addition to a list of basic
blocks, the 
\bb{MachineFunction} contains a \bb{MachineConstantPool}, a
\bb{MachineFrameInfo}, a \bb{MachineFunctionInfo}, and a
\bb{MachineRegisterInfo}. See
\verb+include/llvm/CodeGen/MachineFunction.h+ for more information. 

\subsection{MachineInstr bundles}
LLVM code generator can model sequences of instructions as \bb{MachineInstr
  bundles}. A MI bundle can model a VLIW group/pack which contains an
arbitrary number of parallel instructions. It can also be used to model a
sequential list of instructions (potentially with data dependencies) that
cannot be legally separated (e.g. ARM Thumb2 IT blocks). Conceptually a MI
bundle is a MI with a number of other MIs nested within:  

\section{LLVM ``MC'' layer}
The \bb{MC layer} is to represent and process code at the raw machine code
level, devoid of {\em high-level\/} information like \bb{constant pools},
\bb{jump tables}, \bb{global variables} or anything like that. At this level,
LLVM handles things like label names, machine instructions, and sections in
the object file. The code in this layer is used for a number of important
purposes: the tail end of the code generator uses it to write a \bb{.s} or
\bb{.o} file, and it is also used by the llvm-mc tool to implement standalone
machine code assemblers and disassemblers.  
This section describes some of the important classes. There are also a number of important subsystems that interact at this layer, they are described later in this manual.

\subsection{MCStreamer API}

\subsection{MCContext class}
The \bb{MCContext} class is the owner of a variety of uniqued data structures
at the MC layer, including \bb{symbols}, \bb{sections}, etc. As such, this is
the class that you interact with to create symbols and sections. This class
can not be subclassed.

\subsection{MCSymbol class}
The \bb{MCSymbol} class represents a \bb{symbol} (a.k.a. \bb{label}) in the
assembly file. There are two interesting kinds of symbols: \bb{assembler
  temporary symbols}, and \bb{normal symbols}. 
\bb{Assembler temporary symbols} are used and processed by the assembler but
are discarded when the object file is produced. The distinction is usually
represented by adding a prefix to the label, for example ``L'' labels are
assembler temporary labels in MachO.  

\bb{MCSymbols} are created by \bb{MCContext} and uniqued there. This means that
\bb{MCSymbols} can be compared for pointer equivalence to find out if they are
the same symbol. Note that pointer inequality does not guarantee the labels will
end up at different addresses though. It’s perfectly legal to output something
like this to the .s file:
\begin{verbatim}
  foo:
  bar:
    .byte 4
\end{verbatim}
In this case, both the foo and bar symbols will have the same address.

\subsection{MCSection class}
The \bb{MCSection} class represents an object-file specific section. It is
subclassed by object file specific implementations (e.g. \bb{MCSectionMachO},
\bb{MCSectionCOFF}, \bb{MCSectionELF}) and these are created and uniqued by
\bb{MCContext}. The \bb{MCStreamer} has a notion of the current section, which can be
changed with the \bb{SwitchToSection} method (which corresponds to a ”.section”
directive in a .s file). 

\subsection{MCInst class}



\section{Initialization}
\subsection{sys::printStackTraceOnErrorSignal()}
\bit
\w register \bb{printStackTrace} as a signal handler
\eit
\subsection{PrettyStackTraceProgram}
\bit
\w add \bb{CrashHandler} as another signal handler
\eit
\subsection{getGlobalContext}
\bit
\w \verb+LLVMContext &context = getGlobalContext()+
\w \bb{LLVMContext}: container of ``global'' state in LLVM (e.g. global type
and constant uniquing tables) (\bb{ManagedStatic} object)
\eit

\subsection{llvm\_shutdown\_obj}
\bit
\w \verb+llvm_shutdown_obj Y+: allocate object on stack (so that it will be
deallocated on exit from this stack frame)
\w dtor of this object contains the \verb+llvm_shutdown()+ function to be
called when exit
   \bit
   \w deallocate and destroy all \bb{ManagedStatic} objects 
   \eit
\w \bb{ManagedStatic}:  lazily constructed on demand (goof for reducing
   startup times of dynamic libraries that link to LLVM components) and makes
   destruction to be explicit through the \verb+llvm_shutdown()+ function call
\eit

\subsection{Initialize Targets, TargetMCs, AsmPrinters, AsmParsers}
\bit
\w \bb{Target}:
  \bit
  \w e.g. Sparc, X86, PowerPC, MBlaze, MSP430
  \eit
\w \bb{TargetMC}:
\w \bb{AsmPrinter}:
\w \bb{AsmParser}:
\eit

\subsection{cl::ParseCommandLineOptions}

\subsection{SMDiagnostic Err}
\subsection{std::auto\_ptr Module M}

\subsection{M.reset(ParseIRFile(inputfilename, Err, Context))}
\bit
\w \bb{MemoryBuffer}: simple read-only access to block of memory and provides
methods for reading FILES or STDIN into a memory buffer
\w \bb{ParseIRFile} returns \bb{Module*}, which is set to \verb+M+
\w \verb+OwningPtr<MemoryBuffer> File;+
\w \verb+MemoryBuffer::getFileOrSTDIN(filename.c_str(), File)+
\eit

\subsection{Triple TheTriple(mod.getTargetTriple())}


\subsection{GetOutputStram}

\subsection{PassManager PM}
\bit
\w \verb+PM = new PassManagerImpl();+
\w \verb+PM->setTopLevelManager(PM);+
\w \bb{PassManager} manages a tree of \bb{Pass}es, where its implemenation is delegated to \bb{PassManagerImpl}
\w \verb+PassManagerImpl::add(Pass *P)+
\w \bb{Pass}: interface implemented by all ``passes''; there are multiple
subclasses of passes (e.g. \bb{BasicBlockPass}, \bb{FunctionPass},
\bb{ModulePass}, etc.)

\w see \bb{PassManagers.h} for comments
\eit

\subsection{PM.add(TargetData(*Target.getTargetData()))}

\subsection{PM.Run or PassManager::run(Module \&M)}
\bit
\w \bb{dumpArguments()}
\w \bb{dumpPasses()}
\w \bb{initializeAllAnalysisInfo()}
  \bit
  \w \bb{PassManagers} contains a vector of \bb{PMDataManager}  which manages
  the analysis data used by pass managers
  \eit
\w \verb+getContainedmanager(idx)->runOnModule(M);+
\eit

\section{Passes in -O3}
\subsection{Optimization Passes}
\bit
\w Target Data Layout
\w No Alias Analysis (always returns 'may' alias)
\w Type-Based Alias Analysis
\w Basic Alias Analysis (stateless AA impl)
\w Create Garbage Collector Module Metadata
\w Machine Module Information
\w  ModulePass Manager
   \bit
   \w FunctionPass Manager
     \bit
     \w Preliminary module verification
     \w Dominator Tree Construction
     \w Module Verifier
     \w Natural Loop Information
     \w Loop Pass Manager
       \ben
       \w Canonicalize natural loops
       \een
     \w Scalar Evolution Analysis
     \w Loop Pass Manager
       \ben
       \w Canonicalize natural loops
       \w Induction Variable Users
       \w Loop Strength Reduction
       \een
     \w Lower Garbage Collection Instructions
     \w Remove unreachable blocks from the CFG
     \w Exception handling preparation
     \w Optimize for code generation
     \w Insert stack protectors
     \w Preliminary module verification
     \w Module Verifier
   \eit
  \eit
\eit
\subsection{Code Generation Passes}
\bit
\w Machine Function Analysis
\w X86 DAG$\rightarrow$DAG Instruction Selection
\w X86 PIC Global Base Reg Initialization
\w Expand ISel Pseudo-instructions
\w Optimize machine instruction PHIs
\w Local Stack Slot Allocation
\w Remove dead machine instructions
\w MachineDominator Tree Construction
\w Machine Natural Loop Construction
\w Machine Instruction LICM
\w Machine Common Subexpression Elimination
\w Machine code sinking
\w Peephole Optimizations
\w Tail Duplication
\w X86 Maximal Stack Alignment Check
\w Remove unreachable machine basic blocks
\w Live Variable Analysis
\w MachineDominator Tree Construction
\w Machine Natural Loop Construction
\w Eliminate $\phi$-nodes for register allocation
\w Two-Address instruction pass
\w Process Implicit Definitions
\w Slot index numbering
\w Live Interval Analysis
\w Debug Variable Analysis
\w Simple Register Coalescing
\w Calculate spill weights
\w Live Stack Slot Analysis
\w Virtual Register Map
\w Linear Scan Register Allocator
\w Stack Slot Coloring
\w Machine Instruction LICM
\w Bundle Machine CFG Edges
\w X86 FP Stackifier
\w Subregister lowering instruction pass
\w Prolog/Epilog Insertion \& Frame Finalization
\w Post RA top-down list latency scheduler
\w Control Flow Optimizer
\w Tail Duplication
\w Analyze Machine Code For Garbage Collection
\w MachineDominator Tree Construction
\w Machine Natural Loop Construction
\w Code Placement Optimizer
\w SSE execution domain fixup
\w Machine Natural Loop Construction
\w X86 AT\&T-Style Assembly Printer
\w Delete Garbage Collector Information
\eit


\section{Analysis Passes}
\subsection{Alias Analysis}
\bit
\w used to determine if a storage location may be accessed in more than one
way; two pointers are \bb{aliased} if they point to the same memory location 
\w typically AA respond to a query with \bb{must, may,} or \bb{no} response
\w many different variations:
  \bit
  \w \bb{type-based alias analysis}:
     in {\em type-safe\/} languages where pointers to {\em local\/} variables
     are not allowed (e.g. ML, Haskell, or Java), some useful optimization can be
     made 
   \w \bb{context-sensitive vs context-insensitive}
   \w \bb{flow-sensitive vs flow-insensitive}
   \w \bb{field-sensitive vs field-insensitive}
   \w \bb{unification-based vs subset-based}
   \eit
\eit
\subsubsection{Basic Alias Analysis (Stateless AA Impl)}
\bit
\w File: \verb+/lib/Analysis/BasicAliasAnalysis.cpp+
\w Option: \verb+-basicaa+
\w basic alias analysis pass that implements identities (two different globals
cannot alias, etc.) but does not perform stateful analysis
\w determines whether or not separate memory references point to the same
location of the memory 
\eit

\subsubsection{ScalarEvolution-based Alias Analysis}
\bit
\w Option: \verb+-scev-aa+
\w simple alias analysis implemented in terms of ScalarEvolution queries
\w this differs from traditional \bb{loop dependence analysis} in that it
tests for dependencies within a single iteration of a loop, rather than
dependencies between different iterations
\w ScalarEvolution has a more complete understanding of pointer arithmetic
than Basic Alias Analysis' collection ad-hoc analyses
\eit

\subsubsection{Exhaustive Alias Analysis Precision Evaluator}
\bit
\w Option: \verb+-aa-eval+
\w this is a simple $O(n^2)$ alias analysis accuracy evaluator
\w for each function in the program, it simply queries to see how the alias
analysis implementation answers alias queries between each pair of pointers in
the function 
\eit
\subsubsection{Count Alias Analysis Query Reponses}
\bit
\w Option: \verb+-count-aa+
\w a pass which can be used to count how many alias queries are being made and
how the alias analysis implementation being used responds
\eit

\subsubsection{Alias Analysis Use Debugger}
\bit
\w Option: \verb+-debug-aa+
\w this simple pass checks alias analysis users to ensure that if they create
a new value, they do not qeury Alias Analysis (AA) without informing it of the
value
\w it acts as a shim over any other Alias Analysis pass you want
\w keeping track of every value in the program is expensive but this is a
debugging pass
\eit

\subsection{Call Graphs}
\subsubsection{Basic Call Graph Construction}
\bit
\w Option: \verb+-basiccg+
\w 
\eit

\subsection{Dominator Trees}
\subsubsection{Dominance Frontier Construction}
\bit
\w Option: \verb+-domfrontier+
\w domniator construction algorithm for finding forward dominator frontiers
\eit
\subsubsection{Dominator Tree Construction}
\bit
\w File: \verb+lib/VMCore/Dominator.cpp+
\w Option: \verb+-domtree+
\w given an entry block $S$, block $B_1$ \bb{dominates} block $B_2$ if every
$S \rightarrow B_2$ path has to pass through $B_1$
 \bit
 \w entry block dominates all blocks
 \w IDom (immediate dominator) of $B$ :w
 \eit
\eit

\subsection{Loop Analysis}
\subsubsection{Natural Loop Information}
\bit
\w Option: \verb+-loops+
\w File: \verb+Analysis/LoopInfo.h+
\w identify natural loops and determine the loop depth of various nodes of the
CFG 
\w calculates teh nesting structure of loops in a function: for each natural
loop identified, this analysis identifies natural loops contained entirely
within the loop and the basic blocks that make up the loop
\w \bb{natural loop}: has exactly one entry block (called \bb{header}) and
possible several back edges (called \bb{latches}) leading to the header from
inside the loop 
\w natural loops may be several loops that share the same header node
\eit

\subsubsection{Scalar Evolution Analysis}
\bit
\w Option: \verb+-scalar-evolution+
\w used to analyze and categorize scalar expressions in loops
\w specializes in recognizing generaal induction variables, representing them
with the abstract and opaque \verb+SCEV+ class
\w given this analysis, trip counts of loops and other important properties
can be obtained
\w Benefit: this analysis is primarily useful for {\em induction variable
  subtitution\/} and {\em strength reduction\/}
\eit


\section{Transform Passes}
\subsection{Dead Code Elimination}
\subsubsection{Dead Argument Elimination}
\subsubsection{Dead Instruction Elimination}
\bit
\w Option: \verb+-die+
\w performs a single pass over the function, removing instructions that are
obviously dead 
\eit
\subsubsection{Dead Store Elimination}
\bit
\w Option: \verb+-dse+
\w trival dead store elimination that only considers basic-block local
redundant stores
\eit

\subsubsection{Dead Global Elimination}
\bit
\w Option: \verb+-globaldce+
\w  eliminates unreachable internal globals from the program
\w uses an aggressive algorithm, searching out globals that are known to be
alive; after it finds all of the globals which are needed, it deletes whatever is
left over (this allows it to delete recursive chunks of program which are
unreachable) 
\eit

\subsubsection{Aggressive Dead Code Elimination}

\subsection{Scalar Replacement of Aggregates (DT)}
\bit
\w File: \verb+lib/Transform/Scalar/ScalarReplAggregates.cpp+
\w Option: \verb+scalarrepl+
\w breaks up alloca instructions of aggregate type (structure or array) into
individual instructions for each member (if possible); then, if possible, it
transforms the individual alloca instructions into clean scalar SSA form;
combines a simple SRoA algorithm with the Mem2Reg algorithm because often
interact especially for C++ programs; as such iterating between SRoA, then
Mem2Reg until we run of the things to promote works well
\w Example: given a structure-type variable
\begin{verbatim}
  typedef struct { 
    int x;
    int y;
  } point_t;
  point_t p;
  if (p->x > p->y) ...;
\end{verbatim}
we transform this into
\begin{verbatim}
  int px = p->x;
  int py = p->y;
  if (px > py) ...;
\end{verbatim}
\w Caveat: applicable only if target component and the aggregate are not
aliased
\w Benefit:  enables scalar-based optimizations and facilitates {\em register
allocation\/} (since components are smaller than the whole and more fittable into
registers) and {\em constant/copy propagation\/}
\eit


\section{Utility Passes}
\subsection{Preliminary Module Verification}
\bit
\w File: \verb+lib/VMCore/Verifier.cpp+
\w Option: \verb+-preverify+
\w checks if \verb+Function+ object contains at least one \verb+BasicBlock+
object in the iterator
   \bit
   \w \verb+Function+ consists of \verb+BasicBlocks+
   \eit
\eit

\subsection{Module Verification}
\bit
\w File: \verb+lib/VMCore/Verifier.cpp+
\w Option: \verb+-verify+
\w used for validity checking of input to the system; this does not provide
full ``Java style'' security and verification; instead, it just tries to
ensure that code is well-formed, like those listed below
\eit
  \ben
  \w both of a binary operator's operands are of the same type
  \w verify that the indices of mem access instructions match other operands
  \w verify that arithmetic and other things are only performed on first-class
  types 
  \w verify that shifts and logicals only happen on integrals f.e.
  \w all of the constants in a switch statements are of the correct type
  \w the code is in valid SSA form
  \w it should be illgegal to put a lable into any other type (like a
  structure) or to return one (except constant arrays)
  \w only $\phi$-nodes can be self referential:
  \verb+add i32 %0, %0 ; <int>:0+ is bad 
  \w $\phi$-nodes must have an entry for each predecessor, with no extras
  \w $\phi$-nodes must be the first thing in a basic block, all grouped together
  \w $\phi$-nodes must have at least one entry
  \w all basic blocks should only end with terminator insts, not contain them
  \w the entry node to a function must not have predecessors
  \w all Instructions must be embedded into a basic block
  \w functions cannot take a void-typed parameter
  \w verify that a function's argument list agrees with it's declared type
  \w it is illegal to specify a name for a void value
  \w it is illegal to have a internal global value with no initializer
  \w it is illegal to have a ret instruction that returns a value that does
  not agree with the function return value type
  \w function call argument types match the function prototype
  \w all other things that are tested by asserts spread about the code
  \een


\subsection{Loop Optimization}


\section{Code Generation}
\subsection{Optimize for Code Generation}
\subsection{Insert Stack Protectors}
\bit
\w If stack smashing protection required (based on heuristic which considers
the size of buffer)
\w only when stackprotec function attribute is given 
\w NOTE: wiki \bb{Buffer overflow protection}  for details
\eit

\subsection{Machine Function Analysis}
\subsection{X86 DAG$\rightarrow$DAG Instruction Selection}
\subsection{X86 PIC Global Base Reg Initialization}
\subsection{Expand ISel Pseudo-instructions}
\subsection{Optimize machine instruction PHIs}
\subsection{Local Stack Slot Allocation}
\subsection{Remove dead machine instructions}
\subsection{MachineDominator Tree Construction}
\subsection{Machine Natural Loop Construction}
\subsection{Machine Instruction LICM}
\subsection{Machine Common Subexpression Elimination}
\subsection{Machine code sinking}
\subsection{Peephole Optimizations}
\subsection{Tail Duplication}
\subsection{X86 Maximal Stack Alignment Check}
\subsection{Remove unreachable machine basic blocks}
\subsection{Live Variable Analysis}
\subsection{MachineDominator Tree Construction}
\subsection{Machine Natural Loop Construction}
\subsection{Eliminate $\phi$-nodes for register allocation}
\subsection{Two-Address instruction pass}
\subsection{Process Implicit Definitions}
\subsection{Slot index numbering}
\subsection{Live Interval Analysis}
\subsection{Debug Variable Analysis}
\subsection{Simple Register Coalescing}
\subsection{Calculate spill weights}
\subsection{Live Stack Slot Analysis}
\subsection{Virtual Register Map}
\subsection{Linear Scan Register Allocator}
\subsection{Stack Slot Coloring}
\subsection{Machine Instruction LICM}
\subsection{Bundle Machine CFG Edges}
\subsection{X86 FP Stackifier}
\subsection{Subregister lowering instruction pass}
\subsection{Prolog/Epilog Insertion \& Frame Finalization}
\subsection{Post RA top-down list latency scheduler}
\subsection{Control Flow Optimizer}
\subsection{Tail Duplication}
\subsection{Analyze Machine Code For Garbage Collection}
\subsection{MachineDominator Tree Construction}
\subsection{Machine Natural Loop Construction}
\subsection{Code Placement Optimizer}
\subsection{SSE execution domain fixup}
\subsection{Machine Natural Loop Construction}
\subsection{X86 AT\&T-Style Assembly Printer}
\subsection{Delete Garbage Collector Information}


\section{LLVM Code}
\subsection{LLVM Core Classes}
\subsubsection{Value}
\bit
\w base class of all values computed by a program that may be used as operands
to other values 
\w superclass of \bb{Instruction} and \bb{Function} classes
\w each \bb{Value} has a \bb{Type}
\w each \bb{Value} has a ``Use List'' which keeps track of which other Values
are using this Value
\eit
\subsubsection{Use}
\bit
\w \bb{Use} represents an operand of \bb{Instruction} or some other \bb{User}
instance which refers to a \bb{Value}
\eit
\subsubsection{User}
\subsubsection{LLVMContext}
\bit
\w Owns and manages the core ``global'' data of LLVM (e.g. type and constant
uniquing tables) 
\w No locking guarantees; so, be careful to have one context per thread
\eit

\subsubsection{Module}
\bit
\w a Module instance is used to store all the information related to an LLVM
module
\w  an LLVM module is a top-level container of all other LLVM IR objects 
\w each module directly contains
  \bit
  \w a list of \bb{global varaibles}
  \w a list of \bb{funtions}
  \w a list of \bb{libraries} that this module depends on
  \w \bb{symbol table}
  \w various data about target's characteristics
  \eit
\eit


\subsection{LLVM Support Classes}
\subsubsection{ManagedStatic}
\bit
\w Allows lazy on-demand creation of global statics.
\w good for reducing startup time
\w when user uses the given static object \verb+C &operator*()+ and
\verb+C *operator->()+, we construct the object at that time.
\eit

\subsection{MemoryBuffer}
\bit
\w this interface provides simple read-only access to a block of memory, and
provides simple methods for reading files and standard input  into a memory
buffer.  In addition to basic access to the characters in the  file, this
interface guarantees you can read one character past the end of  the file, and
that this character will read as NULL
\w  The NULL guarantee is needed to support an optimization -- it's intended
to be more efficient for clients which are reading all the data to stop
reading when they encounter a NULL than to continually check the file position to see if it has reached the end of the file
\eit

\subsubsection{Mutex, SmartMutex}
\bit
\w supports methods: \bb{acquire}, \bb{release}, \bb{tryacquire}
\w \bb{SmartMutex} is a mutex with a compile-time constant parameter that
indicates whether this mutex should become a NO-OP when we're not running in
multithreaded mode
\eit

\subsection{MemoryFence}
\bit
\w 
\eit

\subsection{ThreadLocal}
\bit
\w 
\eit

\subsection{OwingPtr}
\bit
\w \bb{OwningPtr} mimics a builtin pointer except that it guarantees deletion
of the object pointed to, either on destruction of the \bb{OwningPtr} or via an
explicit \verb+reset()+. Once created, ownership of the pointee object can be
taken away from OwningPtr by using the \verb+take()+ method.
\eit

\subsection{STL Utility Classes}
\subsubsection{auto\_ptr}
\bit
\w provides some basic \bb{RAII} (Resource Acquisition is Initialization) for
C++ raw pointers. 
  \bit
  \w RAII is essential for writing exception-safe C++ programs
  \eit
\eit

\end{document}

% LocalWords:  LLVM Impl basicaa globals stateful aa eval Dominator domtree DT
% LocalWords:  scalarrepl alloca SRoA Mem
