\documentclass{myproc}
\usepackage{times,mydef,myenv}
\begin{document}
\small
\noindent{\large\bf{}Parallel Computing}

\paragraph{Bit-level parallelism\/}
    \bit
    \w  4-bit $\ra$ 8-bit $\ra \cdots \ra $ 64-bit 
    \w word-width increase will be necessary only for larger address space
      (not for performance increase)
    \eit
\paragraph{Instruction-level parallelism}
    \bit
    \w pipelining (arrival of RISC)
    \w ``wide-issue'' \bb{superscalar architecture}
    (fetch multiple instruction at a time and issue
    them in parallel to distinct function units whenever possible)
      \bit
      \w H/W responsible for parallelism
      \eit
    \w \bb{VLIW architecture}: compiler responsible for parallel scheduling
    \w {\em instruction-level parallelism worthwhile only if processor can be
      supplied with instructions and data fast enough to keep it busy}
      \bit
      \w need to avoid control/structural/data hazards 
      \w as a result, more sophisticated branch prediction 
      \w sophisticated caches to avoid the latency of cache misses
      \w out-of-order instruction completion
      \w larger window of instructions that are waiting to issue is maintained 
         (issued as soon as a needed result is produced by a preceding
      instruction) 
      \w \bb{all of these puts a heavy demand on chip resources and, also,
	very heavy design cost}
      \eit
    \eit
\paragraph{Thread-level parallelism}
\bit
\w Parallelism through threads.
\eit

\paragraph{Process-level parallelism}
\bit
\w Parallelism through \bb{interprocess communication}
\eit

\paragraph{Jargons of parallel programming}
\bit
\w \bb{task}: a sequence of instructions that operate together as a group
\w \bb{unit of execution (UE)}: task needs to be mapped to a UE such as a
thread or a process
\w \bb{processing element (PE)}: a hardware element that executes a stream of
tasks (e.g. in a cluster of SMP workstations, one workstation is a PE; in
CUDA, one core can be a PE)
 \[ \mbox{\bb{tasks}} \stackrel{\mbox{\scriptsize{}map}}{\longrightarrow} \mbox{\bb{UEs}}\stackrel{\mbox{\scriptsize{}map}}{\longrightarrow} \mbox{\bb{PEs}} \]

\w \bb{load balancing}: how task--PE mapping is performed in a balanced way 
\eit

\paragraph{Why parallel architecture}
\bit
\w performance of single processor machine is hitting the ceiling; need for a
new paradigmic change
\w Three types of parallel computers
  \bit
  \w \bb{parallel vector processors (PVP)}
  \w \bb{massively parallel processors (MPP)}
  \w \bb{bus-based symmetric shared memory multiprocessors (SMP)}
  \eit
\eit

\paragraph{Convergence of parallel computers}
\bit
\w \bb{communication architecture}: 
    communciation architecture (just like computer architectures) 1) defines
   communication/synchronization operations (i.e. H/W-S/W interface and
   user-system interface) and 2) defines an organizational structures that
   realize these operations

\w different parallel programming models
      \bit
      \w \bb{shared address}
      \w \bb{message passing}
      \w \bb{data parallel}
      \eit
\eit

\paragraph{What makes parallel programming difficult}
\bit
\w \bb{load balancing}: breaking into {\em equal-sized\/} subtasks
\w \bb{scheduling}: dependencies between subtasks
\w \bb{communication overhead}
\w \bb{time for synchronization}
\eit

\paragraph{Performance}
\bit
\w Performance(X) = 1/ExecutionTime(X)
\w X is $n$-times faster than Y
   \[ \frac{\mbox{Performance(X)}}{\mbox{Performance(Y)}} = 
      \frac{\mbox{ExecutionTime(Y)}}{\mbox{ExecutionTime(X)}} = n\]
\eit

\paragraph{Parallelsim and instructions: Synchronization}
\paragraph{Parallelsim and computer arithmetic: Associativity}
\paragraph{Parallelsim and advanced ILP}
\paragraph{Parallelsim and memory hierarchies: Cache coherency}
\paragraph{Parallelsim and I/O: Redundant arrays of inexpensive disks}



%\bibliographystyle{plain}
%\bibliography{bib/mac,bib/compsys,bib/pl}
%\nocite{CS99,BGS94}
\end{document}
