\documentclass{myart}
\usepackage{mathptm,mydef,myenv}
%\usepackage{MinionPro}
\usepackage[all]{xy}
\begin{document}
%\small
\noindent{\large\bf{}Notes on CUDA Programming Model}

%% \begin{figure*}
%% {\small %\scriptsize
%% \centerline{\begin{tabular}{|r|l|c|c|c|c|}\hline
%% & Tesla & \multicolumn{2}{|c|}{Fermi} & \multicolumn{2}{|c|}{Kepler} \\ \hline
%% \bb{GPU} & & GF100 & GF104 & GK104 & GK110 \\ \hline
%% \bb{capability} & & 2.0 & 2.1 & 3.0 & 3.5 \\ \hline
%% \bb{threads/warp} & & 32 & 32 & 32 & 32 \\ \hline
%% \bb{max warps/SM(X)} & & 48 & 48 & 64 & 64 \\ \hline
%% \bb{max threads/SM(X)} & & 1536 & 1563 & 2048 & 2048 \\ \hline
%% \bb{max thread blocks/SM(X)} & & 8 & 8 & 16 & 16 \\ \hline
%% \bb{32-bit registers/SM(Xx)} & & 32768 & 32768 & 65536 & 65536 \\ \hline
%% \bb{max registers/thread} & & 63 & 63 & 63 & 255 \\ \hline
%% \bb{max registers/thread block} & & 1024 & 1024 & 1024 & 1024 \\ \hline
%% \bb{SM(X)} & &&& & \\ \hline
%% \bb{memory/board} &&&& 8 GB & TBA \\ \hline
%% \bb{memory bandwidth} &&&& 320G Bps & TBA \\ \hline
%% \bb{dynamic parallelism} & &  &  & YES & YES \\ \hline
%% \bb{Hyper-Q} & &  &  & YES & YES \\ \hline
%%   \end{tabular}}}
%% \caption{Comparison of NVIDIA GPU architectures}
%% \end{figure*}

\begin{figure*}
\centerline{\begin{tabular}{|r|r|r|r|r|r|r|r|r|}\hline
\bb{compute capability} & 1.0 & 1.1 & 1.2 & 1.3 & 2.0 & 2.1 & 3.0 & 3.5 \\ \hline
\bb{SM version} & sm\_10 & sm\_11 & sm\_12 & sm\_13 & sm\_20 & sm\_21 & sm\_30 &
sm\_35 \\ 
\bb{threads/warp} & 32 & 32 & 32 & 32 & 32 & 32 & 32 & 32 \\ 
\bb{warps/MP} & 24 & 24 & 32 & 32 & 48 & 48 & 64 & 64 \\
\bb{threads/MP} & 768 & 768&1024 & 1024 & 1536 & 1536 & 2048
& 2048 \\ 
\bb{thread blocks/MP} & 8 & 8 & 8 & 8 & 8 & 8 & 16 &
16\\
\bb{max shared memory/MP} & 16384 & 16384 & 16384 & 16384
& 49152 & 49152 & 49152 & 49152 \\ 
\bb{register file size} & 8192 & 8192 & 16384 & 16384 & 32768 & 32768 & 65536
& 65536 \\ 
\bb{register alloc unit size} & 256 & 256 & 512& 512 & 64 & 64 & 256 & 256 \\ 
\bb{allocation granulatiry} & block & block & block & block & warp & warp & warp &
warp\\
\bb{max thread block size} & 512 & 512 & 512 & 512 & 1024  & 1024  & 1024  &
1024 \\
\bb{shared memory size configurations} & 16384 & 16384 & 16384 & 16384 & 49152 & 49152
& 49152 & 49152 \\
&&  & & & 16384 & 16384 & 16384 & 16384 \\
&&  & & &  &  & 32768 & 32768 \\
\bb{warp register allocation granularities} & & & & & 64 & 64 & 256 & 256 \\
& & & & & 128 & 128 &  &  \\ \hline
\end{tabular}}
\end{figure*}

\paragraph{NVIDIA GPU architectures}
\bit
\w \bb{Kepler}
  \bit
  \w \bb{SMX}: 3X performance/watt compared to SM (1 petaflop of computing in
  10 server racks)
  \w \bb{dynamic parallelism}: 
  \w \bb{hyper-Q}:
  \eit
\w PCIe 3.0: 8GT/s (gigatransfers per second\footnote{The \bb{data transfer
    rate} is defined as \bb{channel width (bits/transfer)} $\times$
  transfers/second = bits/second.})
\eit

\paragraph{Kernels}
\bit
\w A \bb{kernel} is a C function, when called, which is executed $N$ times in
parallel by $N$ different CUDA \bb{threads}. A kernel is defined using the
specifier \verb+__global__+. 
\w The programmer organizes threads into a hierarchy of \bb{grids} of
\bb{thread blocks}. 
\w A \bb{thread block} is a set of concurrent threads that can cooperate among
themselves through barrier synchronization and shared access to a memory space
private to the block.
\w A \bb{grid} is a set of {\em thread blocks\/} that may be executed {\em
  independently\/} and thus may execute in parallel.
\w \bb{execution configuration}: each kernel call can be called with
{\em caller-specific\/} configuration which specifies how threads will be deployed
in CUDA processors. A execution configuration has the form
   \[ \mbox{\tt <<<}\ \ G, B, M, S\ \ \mbox{\tt >>>}, \]
where 
  \bit
  \w $G$ (type \verb+dim3+) specifies the dimension and size of
  \bb{grid} s.t. $G_x * G_y * G_z$ equals the number of \bb{blocks} 
  being launched (for devices of compute capability 1.x, $G.z$ must be
  1). $G$ can be an integer if there are only one dimension in a grid.
   
  \w $B$ (type \verb+dim3+) specifies the dimension and size of each
  \bb{block} s.t. $B_x * B_y * B_z$ equals the number of \bb{threads}
  per block. $B$ can be an integer if there are only one dimension in a block.
  \w optional $M$ (type \verb+size_t+), specifies the number of {\em
    bytes\/} in shared memory that is {\em dynamically allocated per block\/}
  for this call in addition to the statically allocated memory. Default value
  is 0 and $M$ must not exceed the amount of shared memory available.
  \w optional $S$ (type \verb+cudaStream_t+) specifies the associated
  stream. Default value is 0.
  \eit
\eit

\paragraph{Thread hierarchy}
\bit
\w \verb+threadIdx+: 3-component vector variable
   which specifies the unique location of a thread inside a \bb{thread block}.
   This variable is {\em a per-thread variable\/} and each thread will have
   a different value which is unique w.r.t. a block.
\w A \bb{thread ID}, a number unique to a thread within a block, can be
computed from the value of \verb+threadIdx+.
  \bit
  \w In 2-dimensional block of size $(B_x, B_y)$, the thread id is $x + yD_x$.
  \w In 3-dimensional block of size $(B_x, B_y, B_z)$, the thread id
  is $x + yD_x + zD_xD_y$.
  \eit

\w \bb{example}: There are two SMs and we can schedule 4 blocks to each SM.
Within a thread block, we can have 16 threads.
{\small
\[ \xymatrix@-1.8pc{
  & SM_0 & SM_1 & & \\ 
  \txt{Grid} &  *+[F]{B_{0,0}} & *+[F]{B_{1,0}} &&\\
  &*+[F]{B_{0,1}} & *+[F]{B_{1,1}} &&\\
  &*+[F]{B_{0,2}} & *+[F]{B_{1,2}} &&\\
  &*+[F]{B_{0,3}} \ar@{.}[ddd] & *+[F]{B_{1,3}} &&\\ \\ \\
  \txt{Block $B_{0,3}$}& *+[F]{T_{0,0}} & *+[F]{T_{1,0}} &  *+[F]{T_{2,0}} & *+[F]{T_{3,0}} \\
  &*+[F]{T_{0,1}} & *+[F]{T_{1,1}} &  *+[F]{T_{2,1}} & *+[F]{T_{3,1}} \\
  &*+[F]{T_{0,2}} & *+[F]{T_{1,2}} &  *+[F]{T_{2,2}} & *+[F]{T_{3,2}} \\
  &*+[F]{T_{0,3}} & *+[F]{T_{1,3}} &  *+[F]{T_{2,3}} & *+[F]{T_{3,3}}
}\]
\w \bb{kernel invocation example}:
  \begin{verbatim}
  dim3 threadsPerBlock(4, 4);  
  dim3 blocksInGrid(N/threadsPerBlock.x,
                    N/threadsPerBlock.y);
  add<<<blocksPerGrid, threadsPerBlock>>>(...);
  \end{verbatim}
}
\eit

\paragraph{Kernel grid execution}
\bit
\w When a CUDA program on the host CPU invokes a kernel grid, the \bb{CWD
  (compute work distribution) unit} enumerates the blocks of the grid and
begins distributing them to SMs with available execution capacity.
\w The threads of a {\em thread block\/} execute concurrently on one SM.
\w As thread blocks terminate, the CWD unit launches new blocks on the
vacated SMs.
\w \bb{Tesla architecture example}:
  \bit
  \w One SM creates, manages, executes up to 768 concurrent threads in hardware
  with zero scheduling overhead.
  \w It can execute up to 8 CUDA thread blocks concurrently.
  \w \bb{single-instruction, multiple-thread (SIMT)}
  \w One thread is mapped to one SP scalar core.
  \w Each scalar thread executes independently with its own instruction
  address and register state. 
  \w \bb{SM SIMT} unit creates, manages, schedules, and executes threads in
  groups of 32 parallel threads called \bb{warps}. 
  \w Each SM manages a pool of 24 warps of 32 threads, a total of 768
  threads. 
  \w Every \bb{instruction issue time}, the SIMT unit selects a warp that is
  ready to execute and issues the next instruction to the active threads of
  the warp. 
  \w A warp executes one common instruction at a time, so full efficiency is
  realized when all 32 threads of a warp agrees on their execution path. 
  \eit
\eit

\paragraph{Memory hierarchy}
\bit
\w A thread has a \bb{per-thread local memory}.
\w A thread block has a \bb{per-block shared memory}, which is shared by all
threads in the thread block.
\w A grid (or multiple grids) has a \bb{global memory}, which is shared by all
threads with grids.
\w In CUDA programming model, there are two memory spaces: \bb{host memory}
and \bb{device memory}. The device memory is further classified into
\bb{global memory}, \bb{constant memory}, and \bb{texture memory}, which are
visible to kernels through calls to CUDA runtime.
\eit

\paragraph{CUDA programming interface}
\bit
\w The \bb{CUDA driver API}  is the lowest-level  API which is used by \bb{CUDA
  runtime API}. Users can also use this API for fine-control over the devices.
  It exposes low-level concepts such as \bb{CUDA contexts} (similar to
  processes in the host) and \bb{CUDA modules} (similar to DLLs). 
\eit

\paragraph{NVCC compilation}
\bit
\w The \verb+nvcc+ compilation flow is as follows.
   \ben
   \w Separate the device code from host code.
   \w Compile the device code into assembly form (\bb{PTX code}) and/or binary
   form (\bb{cubin} object). 
   \w Instrument the host code so that execution configuration
   (\verb+<<<...>>>+) is replaced with CUDA runtime function calls to load and
   launch each compiled kernel from the PTX code and/or cubin object.
   \een
\w Instrumented host code can be either compiled by \verb+nvcc+ or manually by
a user-defined tool. Or, users can directly control the device code using CUDA
driver API to load and execute PTX code or cubin object.
\w \bb{JIT compilation}: PTX code loaded by an application at runtime is
compiled into binary code by the device driver on the fly. This JIT
compilation increases load time, but allows applications to benefit from
latest compiler improvements. 
\w Device driver maintains the \bb{compute cache} which is used to cache a
copy of binary code generated from PTX code through JIT compilation. JIT
compilation can be controlled by following environment variables
  \bit
  \w \verb+CUDA_CACHE_DISABLE+: disable cache
  \w \verb+CUDA_CACHE_MAXSIZE+: specifies the cache in bytes (default is 32MB
  and maximum is 4GB)
  \w \verb+CUDA_CACHE_PATH+: the folder where the cache files are stored
  (default is \verb+~/.nv/ComputeCache+ on Linux)
  \w \verb+CUDA_FORCE_PTX_JIT+: when 1, forces the device driver to ignore any
  binary code embedded in an application but to JIT compile embedded PTX code
  instead 
  \eit
\eit

\paragraph{PTX compatibility}
\bit
\w Some PTX instructions are only supported on devices of higher \bb{compute
  capability}. For example, {\em atomic instructions on global memory\/} are
supported on devices of compute capability of 1.1 and above. 
\w The \verb+-arch+ compiler option specifies the compute
capability that is assumed when compiling C to PTX code.  For example, the
code that contains double-precision arithmetic must be compiled with
``\verb+-arch=sm_13+'' option.
\eit

\paragraph{Application compatibility}
\bit
\w Which PTX and binary code gets embedded in a CUDA C application is
controlled by the \verb+-arch+/\verb+-code+ options or the \verb+-gencode+
option. 
\w For example, the compiler option
  \begin{verbatim}
   -gencode arch=compute_10,code=sm_10
  \end{verbatim}
embeds binary code compatible with compute capability 1.0.
\eit

\paragraph{CUDA C runtime}
\bit
\w The runtime is implemented in the \verb+cudaart+ dynamic library which is
typically included in the application installation package. All its entry
points are prefixed with \verb+cuda+.
\w The CUDA runtime is \bb{initialized} when the first runtime function call
happens. During initialization, the runtime creates a \bb{CUDA context} for
each device, which is called the \bb{primary context} for the device shared
among all the host threads of the application. 
\eit

\paragraph{Device memory}
\bit
\w Device memory can be allocated either as \bb{linear memory} or as \bb{CUDA
  arrays}. 
\w Linear memory is typically allocated and freed using \verb+cudaMalloc()+ and
\verb+cudaFree()+, respectively. The data transfer between host and device
memory is done using \verb+cudaMemcpy()+.  
  \bit
  \w Linear memory can also be allocated through \verb+cudaMallocPitch()+ and
  \verb+cudaMalloc3D()+, for allocations of 2D or 3D arrays as it makes sure
  that the allocation is appropriately \bb{padded} to meet the alignment
  requirements. Also, \verb+cudaMemcpy2D()+ and \verb+cudaMemcpy3D()+ are
  used for data transfer.
  \eit
\w As for the linear memory, Devices of compute capability 1.x has 32-bit
address space (4GB). Devices of compute capability have 40-bit address space (1TB). 
\eit

\paragraph{Shared memory}
\bit
\w A shared memory is allocated using the \verb+__shared__+ qualifier.
\eit


\paragraph{Constant memory}

\paragraph{Texture memory}



\paragraph{Overview}
\bit
\w suitable for \bb{data-parallel computation}: the same program executed on
many data elements in parallel (with \bb{high arithmetic intensity} -- the
ratio of arithmetic operations to memory operations)
\w NVIDIA device consists of multiple \bb{streaming multiprocessors (SM)}
\w \bb{SM} consists of 8 \bb{scalar thread processors (SP)}
\eit

\paragraph{NVIDIA GeForce 9500 GT}
\bit
\w CUDA cores: 32 (4 SMs $\times$ 8 cores/SM)
\w memory: 1GB (128-bit interface)
\w bus: PCI express x16 gen1 (max width: x16; max speed: 2.5 GT/s)
\w graphics clock: 550MHz
\w memory clock: 400MHz
\w processor clock: 1375MHz
\w cuda capability: 1.1
\eit

\paragraph{Development environment}
\bit
\w CUDA-enabled GPU
\w NVIDIA device driver: allows communication with GPU
\w CUDA development toolkit
\w standard C compiler
\w \bb{flow}: CUDA program $\ra$ standard C program with CUDA toolkit funcalls
$\ra$ assembler
  
\eit

\paragraph{SIMT thread execution}
\bit
\w \bb{warp} (\bb{``thread-vector''}): a set of parallel threads that execute a single instruction
  \bit
  \w groups of 32 threads formed into \bb{warps}
  \w these threads executes the same instruction
  \w some become inactive when code path diverges
  \w {\em hardware automatically handles divergence}
  \eit
\w \bb{warps are the primitive unit of scheduling}
  \bit
  \w pick 1 of 32 warps for each instruction slot
  \w note that warps may be running different programs/shaders!
  \eit
\w SIMT execution is an {\em implementation choice}
  \bit
  \w sharing control logic leaves more space for ALUs
  \w largely invisible for programmer
  \w must understand for performance, not correctness
  \eit
\eit


\paragraph{Splitting parallel blocks}
\bit
\w \bb{thread block}: 3-dimensional vector; may contain up to 1024 threads (4.2)
   \bit
   \w \bb{blockDim.\{x,y,z\}}: 1/2/3-dimensional \bb{thread block} space
   \w \bb{threadIdx.\{x,y,z\}}: thread index for each block dimension 
   \w two-dimensional block of size $(D_x, D_y)$: thread ID of a thread of
   index $(x, y)$ is $x + yD_x$
   \w three-dimensional block of size $(D_x, D_y, D_z)$: thread ID of a thread
   of index $(x, y, z)$ is $x + yD_x + zD_xD_y$. 
   \eit
\w \bb{grid}: can have 1, 2-dimensional grid of thread blocks
  \bit
  \w \bb{gridDim.\{x,y\}}: 1/2-dimensional \bb{grid} space
  \eit

\eit

\paragraph{Kernel function call}
\bit
\w \bb{kernel}: \verb+__global__+ declaration specifier
\w \verb+add<<nblocks, nthreads_per_block>>(args)+
   \bit
   \w total \# threads: \verb+nblocks+ $\times$ \verb+nthread_per_block+
   \eit
\w \verb+add<<<N, 1>>>(args)+: $N$ threads  all in one block
  \bit
  \w \verb+int tid = blockIdx.x+
  \eit
\w \verb+add<<<1, N>>>(args)+: one thread in $N$ blocks 
      (one thread in each block)
  \bit
  \w \verb+int tid = threadIdx.x+
  \eit
\w given that we want 128 threads per block BUT
  (1) and/or $N$ can be less than 128 and/or
  (2) $N$ is not multiple of 128
  \bit
  \w [(1)] \bb{kernel call}: \verb|add<<<(N+127)/128, 128>>>|
  \w [(2)] \bb{guard inside kernel}:
    \begin{verbatim}
    if (tid < N) { 
       // guard against !(N mod 128)
       c[tid] = a[tid] + b[tid];
    }
    \end{verbatim}
  \eit

  
\eit


\paragraph{Warp divergence}
\bit
\w \bb{given}:
  \begin{verbatim}
  if (x < 0.0)
     z = z - 2.0;
  else
     z = sqrt(x);
  \end{verbatim}
\w \bb{predication}: compute predicate first and then execute two predicated instructions
  \begin{verbatim}
  p = (x < 0.0);
  (p):  z = x - 2.0; // single instruction
  (!p): z = sqrt(x);
  \end{verbatim}
  \bit
  \w both branches are executed; so, large branch (which actually is taken by
  only one thread) code causes all other threads to execute it 
  \eit
\w \bb{warp voting}: if branches are big, \bb{nvcc} inserts code to check if
all threads in the warp  take the same branch
\eit


\paragraph{Memory: Optimize memory access}
\bit
\w coalesced vs non-coalesced = order of magnitude
  \bit
  \w \bb{coalesced memory transaction}: one in which all of the threads in a
  half-warp access global memory at the same time. e.g. if threads 0, 1, 2, 3
  read global memory 0x0, 0x4, 0x8, 0xc, it is a coalesced read.
  \w Example: given an array: 0 1 2 3 4 5 6 8 9 a b
   \begin{verbatim}
   CASE #1: 
     thread 0: 0, 1, 2
     thread 1: 3, 4, 5
     thread 2: 6, 7, 8
     thread 3: 9, a, b
   CASE #2: 
     thread 0: 0, 4, 8
     thread 1: 1, 5, 9
     thread 2: 2, 6, a
     thread 3: 3, 7, b
   \end{verbatim}
    CASE \#2 is a coalesced since the ``first access'' is 0, 1, 2, 3. If threads
    in a block are accessing consecutive global memory locations, then all
    accesses are combined into a \bb{single request} (or \bb{coalesced}) by the
      hardware. 
  \eit
\w optimize for spatial locality in cached texture memory
\w in shared memory, avoid {\em high-degree bank conflicts}
\eit
\paragraph{Memory: Take advantage of shared memory}
\bit
\w hundreds of times faster than global memory
\w threads can cooperate via shared memory
\w {\em use one/a-few threads to load/compute data shared by all threads\/}
\w use it to avoid non-coalesced access: stage loads and stores in shared
memory to re-order non-coalesceable addressing
\eit


\bibliographystyle{plain}
\bibliography{bib/softsys}
\nocite{CUDAPG42}
\end{document}


% LocalWords:  GPU vertices SPMD  GPUs CUDA NVIDA GF NVIDIA SMX petaflop PCIe
%%  LocalWords:  gigatransfers cudaStream threadIdx yD zD xD SMs ddd CWD API
%%  LocalWords:  runtime DLLs NVCC nvcc PTX cubin JIT MAXSIZE sm gencode cuda
%%  LocalWords:  cudaart cudaMalloc cudaFree cudaMemcpy cudaMallocPitch PCI
%%  LocalWords:  GeForce funcalls SIMT shaders ALUs blockDim gridDim nblocks
%%  LocalWords:  nthreads args nthread tid blockIdx xc coalesceable softsys
