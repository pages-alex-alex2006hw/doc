\documentclass{article}
\usepackage{mathptm,theorem,myenv,mydef,newalgstoc}
\begin{document}
\title{\large\bf{}Notes on Parallel Algorithms}
%\author{¡§ √∂¡÷}
\maketitle

\section{Introduction}
\subsection{The PRAM Model}
\bit
\w CRCW PRAM
	\ben
	\w [(a)] {\bf{}Common CRCW PRAM}: all processors writing into the
			same location must write the same value
	\w [(b)] {\bf{}Arbitrary CRCW PRAM}: any one processor participating
		in a common write may succeed, and the algorithm should work 
		correctly
		regardless of which one succeeds
	\w [(c)] {\bf{}Priority CRCW PRAM}: there's a linear ordering on the
		processors, and the minimum numbered processor writes its value
		in a concurre2nt write
	\een
\w {\em{}A priority CRCW PRAM algorithm can be simulated by an EREW PRAM
	with the same number of processors with the parallel time
	within a factor of $O(\lg P)$ where $P$ is the number of processors.}
\w {\em{}A priority CRCW PRAM algorithm can be simulated by a common CRCW
	PRAM with the same parallel time provided that the number of
	processors are sufficiently many.}
\eit
\subsection{Optimality and Efficiency of Parallel Algorithms}
\bit
\w Let a problem whose fastest (known) sequential algorithm $A_s$ runs in 
	$\Gamma$ time be given. And suppose that there's a parallel
	algorithm $A_p$
	which runs in $T$ time with $P$ processors\footnote{Note
	that $\Gamma, T$ and $P$ are all functions of the input size $n$. That
	is, these are abbreivations for $\Gamma(n), T(n)$ and $P(n)$.}.
\w The {\bf{}speedup} $S$ of a parallel algorithm is defined as
		\[ S = \frac{\Gamma}{T}. \]
	\bit
	\w We want as much speedup as possible.
	\w We say that a parallel algorithm achieves {\bf{}linear
		speedup} when $S = P$ or $S = \Theta(P)$.
	\w Linear speedup means that we've got a $P$-processor
		parallel algorithm
		that is $P$ times as fast as the sequential one.
	\w Note that $S \le P$. Since, otherwise, we could devise
		a sequential algorithm that is faster than $A_s$:
		we can change $(T, P)$-algorithm into $(TP, 1)$-algorithm
		by unfolding all the parallel steps; since
		$\Gamma \le TP$, $S = \Gamma/T \le P$.
	\eit
\w The {\bf{}work} $W$ of a parallel algorithm is defined as
	\[ W = TP.\]
	\bit
	\w The work measures the total processing effor needed
		for a parallel algorithm and it {\em{}accounts for
		inefficiencies caused by one or more processors
		being idle.\/}
	\w Alternatively, the work is sometimes defined to be 
		$N_1 + N_2 + \cdots + N_T$, where $N_i$ is the
		number of processors that are actively used during the $i$th
		step.
	\eit
\w The {\bf{}efficiency} $E$ of a parallel algorithm is defined as
	\[ E = \frac{\Gamma}{W}.\]
	\bit
	\w The efficiency measure the efficiency with which the 
		{\em{}processors are utilized\/}.
	\w Alternatively, the efficiency is the ration of the
		speedup to the number of processors used:
	\[ E = \frac{\Gamma}{W} = \frac{\Gamma}{TP} = \frac{S}{P}.\]
	\w Note that $E \le 1$ since $S \le P$.
	\w We want $E$ to be as close to $1$ as possible.
	\eit
\w Define $polylog(n) = \bigcup_{k\le 0}O(\lg^k n)$.
\w A parallel algorithm is {\bf{}optimal} if
	\ben
	\w [(a)] $T = polylog(n)$ and
	\w [(b)] $W = PT = \Gamma$, i.e. $S = P$ or $E = 1$.
	\een
\w A parallel algorithm is {\bf{}efficient} if
	\ben
	\w [(a)] $T = polylog(n)$ and
	\w [(b)] $W = \Gamma\cdot{}polylog(n)$.
	\een
\w {\em{}A major goal in parallel algorithm design is to 
	find optimal and efficient algorithms with $T$ as
	small as possible}.
\eit
\subsection{Brent's Scheduling Principle}
\bit
\w Consider a computation $C$ that can be done in $T$ steps
	with $X_i$ primitive operations at the $i$th step.
\w $C$ can be implemented using a parallel algorithm that runs
	in $T$ time with $M = \mbox{max}_i\{X_i\}$ processors.
\w If we have $P \le M$ processors,  we can still simulate
	the $i$th step of $C$, in time
	$\ceil{X_i/P} \le X_i/P + 1$.
\w ({\bf{}Brent's Lemma})
	Hence the total parallel time to simulate $C$ with $P$ processors
	is no more than 
	$\ceil{X/P} + T$ where $X = \sum_iX_i$, since
	\[ \ceil{X_1/P} + \cdots \ceil{X_T/P} \le X_1/P + \cdots + X_T/P + T
	= X/P + T.\]
\w {\sl\bfseries{}The Brent's lemma is applicable only if 
	the processor allocation is not a problem; i.e., it is possible
	for each of $P$ processors to determine, on-line, the steps
	steps it needs to simulate\/}.
\w When the Brent's lemma is applicable, 
	{\em{}a parallel algorithm requiring work $W$ and time $T$ can be
	simulated using $P$ processors in $W/P + T$ time\/}.
	\bit
	\w In practice, $P <\!< N$, $W/P$ dominates the magnitude
		of $W/P + T$.
	\eit
\eit


\section{Basic PRAM Techniques}
\subsection{Prefix Sums}
\paragraph{Prefix sums problem}
\bit
\w ({\bf{}Prefix Sums Problem}) Let $\oplus$ be an associative
	operation over a domain $D$. Given an ordered set
	\[ A = [a_1, a_2, \cdots, a_{n}] \]
	of $n$ elements from $D$, the prefix sums problem is 
	to compute the $n$ prefix sums
	\[S_i = a_1 \oplus a_2 \oplus \cdots \oplus a_i =
	\bigoplus_{k = 1}^i a_k\]
	for $1 \le i \le n$.
\w There are two variants the problem w.r.t. the representation of $A$:
	\ben
	\w [(a)] {\bf{}Vector prefix sums}: $A$ is represented as an
		{\bf{}array}\footnote{Usually, prefix sums problems 
		assumes an array as its input representation.}; 
		algorithms for this variant is
		called the {\bf{}scan} operation.
		\bit
		\w The {\bf{}prescan} operation takes $A$ in a vector 
			form 
			and returns $[I, a_1, (a_1 \oplus a_2), 
			\cdots, (a_1 \oplus \cdots \oplus a_{n-1})]$ where
			$I$ is the identity.
		\eit
	\w [(b)] Linked list prefix sums: this is more often than not 
		treated as a variation of the {\bf{}list ranking problem}.
	\een
\w Applications of prefix sums:% \cite[chapter 1]{Reif92}
	\ben
	\w To compact a sparse array.
	\w To lexically compare strings of characters.
	\w To add multiprecision numbers
	\w To evaluate polynomials
	\w To solve recurrences.
	\w To implement radix sorts.
	\w To implement quicksort.
	\w To search for regular expressions.
	\w To dynamically allocate processors.
	\w To label components in $2$-dimensional images.
	\een
\w ({\bf{}Segmented Prefix Sums Problem}) Given an array 
	$A = [a_1, a_2, \cdots, a_{n}]$
	of $n$ elements from $D$ 
	with {\em{}flag} array	$F = [f_1, f_2, \cdots, f_n]$,
	where $f_i = 0$ or $1$, the segmented prefix sums problem is 
	to compute the $n$ segmented prefix sums
	\[S_i = \left\{\begin{array}{ll}
		a_1 & \quad i = 1,\\
		a_i & \quad i > 1 \mbox{\ and\ } f_i = 1,\\
		S_{i-1} \oplus a_i & \quad i > 1 \mbox{\ and\ } f_i = 0.
		\end{array}\right.\]
	for $1 \le i \le n$.
	\bit
	\w The segmented prefix sums problem can be reduced to the 
		prefix sums problem as follows:
	\[ S_i \left\{\begin{array}{ll}
		a_1 & \quad i = 1,\\
		(S_{i-1} \times_s f_1) \oplus a_i & \quad i > 0.
		      \end{array}\right. \]
	where
	\[ S \times_s f = \left\{\begin{array}{ll}
		I_\oplus & \quad f = 1,\\
		S & \quad f = 0.
				 \end{array}\right.\]
	\eit
\eit
\paragraph{Parallel prefix sums algorithm}
\bit
\w Assume $n = 2^k$ for some $k > 0$.\vspace{0.2cm}

\begin{minipage}[b]{1cm}
	\begin{algorithm}{ParallelPrefixSums}{[a_1, \cdots, a_n]}
	\begin{IF}{n = 1}
		S_1 \= a_1
	\ELSE
		\begin{FOR}{i \= 1, \cdots, n/2}
			b_1 \= a_{2i-1} \oplus a_{2i}
		\end{FOR}\\
		(B_1, \cdots, B_{n/2}) \= 
		\CALL{ParallelPrefixSums}{([b_1, \cdots, b_{n/2}])}\\
		\begin{FOR}{\mbox{even $i$}}
			S_i \= B_{i/2}
		\end{FOR}\\
		\begin{FOR}{\mbox{odd $i$}}
			S_i \= B_{(i-1)/2}\oplus a_i
		\end{FOR}\\
		\RETURN(S_1, \cdots, S_n)
	\end{IF}
	\end{algorithm}
\end{minipage}
\w {\sf{}ParallelPrefixSums} runs in EREW PRAM since there are no memory
	conflicts.
\w The parallel time satisfies the recurrence 
	$T(n) = T(n/2) + O(1)$ with $T(1) = 1$. Thus,
	\[ T(n) = O(\lg n). \]
\w The work satisfies $W(n) = W(n/2) + O(n)$ with $W(1) = 1$. Thus
	\[ W(n) = O(n).\]
\w Using Brent's lemma, we can see that {\sf{}ParallelPrefixSums} is
	an optimal EREW PRAM algorithm with $P = O(n/\lg n)$ processors.
	\bit
	\w Let $P \le n/\lg n$ processors be given and let
		$Q = \floor{n/P}$.
	\w We assign the $i$th processor to elements
		$a_{(i-1)q + 1}, a_{(i-1)q + 2}, \cdots, a_{iq}$.
	\eit
\eit

\paragraph{Cole \& Vishkin's algorithm for prefix sums}
\bit
\w Cole and Vishkin \cite{CV86b} devised an optimal 
	CRCW PRAM algorithm with
	$T = O(\lg n/\lg\lg n)$ with a restriction that
	$a_i$ are $O(\lg n)$-bit numbers and $\oplus$ is an $O(1)$-time
	operation.
\eit

\subsection{List Ranking}
\paragraph{List ranking problem}
\bit
\w ({\bf{}List Ranking Problem}) Given a linked list of $n$ elements,
	compute the suffix sums of the last $i$ elements of the list,
	$1 \le i \le n$.
\w The list ranking problem can be thought of as a variant of 
	prefix sums problem where $a_i = 1$,
	the associative operator $\oplus$ is $+$, the
	input is represented as a {\sl\bfseries{}linked list\/}, 
	and the sum is computed from the end.
	\bit
	\w That is, this problem is determining the {\em{}rank\/} of
		each elements, where the {\bf{}rank} of an element 
		is defined to be
		the number of elements preceding it in the linked list.
		
	\eit
\eit
\paragraph{List ranking algorithm based on pointer jumping}
\bit
\w Assume that the linked list represented by a contents array $C[1..n]$
	and a successor array $S[1..n]$, where $c(i)$ gives the value
	of the $i$th element and $s(i)$ gives the index of the 
	successor of the $i$th element. \vspace{0.2cm}

\begin{minipage}[b]{4cm}
\begin{algorithm}{BasicListRanking}{C, S}
\begin{REPEAT}{}
	\begin{FOR}{i \= 1, \cdots, n}
		c(i) \= c(i) \oplus c(s(i));\\
		s(i) \= s(s(i));
	\end{FOR}
\end{REPEAT}{\ceil{\lg n} \mbox{iterations}}
\end{algorithm}
\end{minipage}
\w The step~4 is called {\bf{}pointer jumping} (or {\bf{}pointer doubling}
	or {\bf{}shortcutting}).
\w After the execution of {\sf{}BasicListRanking},
	$c(i)$ is the {\em{}distance of the $i$th element from the
	end of the linked list}.
	\bit
	\w Note that we can compute the rank of the $i$th element, $R(i)$, by
		\[ R(i) = c(i) - n + 2.\]
	\eit
\w {\sf{}BasicListRanking} runs 
	in $T = O(\lg n)$ time using $P = O(n)$ processors.
	So, this algorithm is not work-optimal.
\w {\em{}Though this problem seems very similar to the prefix sums
	problem, we cannot apply the same idea of prefix sums algorithm since
	a {\sl\bfseries{}given element has no way of knowing whether it is at 
	an odd or even position on the list.}}
\eit
\paragraph{Sketch of optimal list ranking algorithms}
\bit
\w We need not necessarily locate the elements at `even' positions.
\w {\em{}It suffices to construt a set $S$ of no more than $k\cdot{}n$
	elements in the list, with $k < 1$, such that the distance in the list
	between any two consecutive elements of $S$ is small}.
	\bit
	\w That is, instead of choosing $n/2$ even-position elements,
		we choose $k\cdot{}n$ elements distributed uniformly 
		in the list.
	\eit
\w List ranking algorithm
	\ben
	\w {\bf{}List contraction}: create a contracted list composed of the
		elements in $S$, in which each element of $S$ has as its 
		successor the first element of $S$ that follows it in the
		original list, and a value equal to its own value in the
		original list, plus the sum of the values of the elements
		that lie between it and this successor.
	\w Recursively, solve the list ranking problem for the contracted
		list. The suffix sum for each element in the contracted list
		is the same as its suffix sum in the original list.
	\w Extend this solution to all elements of the
		original list. The time to do this is proportional
		to the maximum distance between two elements of $S$
		in the original list, and the work is 
		proportional to the length of the original list.
	\een
\w Once a contracted list of length less than $n/\lg n$ is obtained,
	list contraction is no longer needed; instead the list
	ranking problem for the contracted list can be solved 
	using {\sf{}BasicListRanking} algorithm.
	\bit
	\w This requires $O(\lg n)$ time using $n/\lg n$ processors.
	\eit
\w {\bf{}How can we construct $S$?}
	\bit
	\w We need to consider two points:
		\ben
		\w How can we choose the set $S$?
		\w How can we compact the elements of $S$ into consecutive
		locations, in preparation for the recursive solution of the
		list ranking problem on the compacted list?
		\een
	\eit
\eit
\paragraph{Constructing $S$ using random mate algorithm}
\bit
\w Each element chooses a gender, {\em{}male\/} or {\em{}female\/} with
	equal probability.
\w {\em{}An element is in set $S$ iff it is female or has a male
	predecessor.\/}
	\bit
	\w Then, with probability $1 - o(1)$, 
		$|S| \le 15n/16$.
	\w Each element in $S$ can find its successor in $S$ in constant
		time, since the distance to its successor is at most $2$.
	\eit
\w With random mating, each list contraction tends to shrink the
	length of the list by a constant factor, and thus the
	number of contractions needed to pass from the original
	list of length $n$ to a list of length less than $n/\lg n$
	is $O(\lg\lg n)$.
\w We can {\em{}compact} $S$ into consecutive locations using
	{\em{}prefix sums\/} in $O(\lg n)$ time.
\eit

\paragraph{An optimal $O(\lg n)$-time deterministic list ranking algorithm}
\bit
\w A {\em{}symmetry breaking\/} technique known as {\bf{}deterministic
	coin tossing} can be used for this purpose.
\w Given an $n$-element list, a subset $S$ of these elements
	is an {\bf{}$r$-ruling set} if $S$ contains no two adjacent
	elements of the list, and every element not in $S$ is at a
	distance no more than $k\cdot{r}$ on the list from an
	element in $S$, where $k$ is a suitable constant.\vspace{0.2cm}

\begin{minipage}[b]{1cm}
	\begin{algorithm}{RulingSetAlgorithm}{C, S, P}
	\begin{FOR}{i \= 1 \TO n}
	c(i) \= i
	\end{FOR}\\
	\REPEAT{}
		\begin{FOR}{\mbox{each $i$}}
		\mbox{\tt{}// $c_q(i)$ denotes the $q$th bit of $c(i)$}\\
		\mbox{\tt{}// $c_q(s(i))$ denotes the $q$th bit of $c(s(i))$}\\
		\mbox{\tt{}// bin$(q)$ denotes the binary representation of 
			$q$}\\
		q \= \mbox{rightmost position $q$ s.t. $c_q(i) \ne
			c_q(s(i))$}\\
		b \= \mbox{\ the $q$th bit of $c(i)$}\\
		c(i) \= \mbox{$b$ concatenated with the bin$(q)$)}
		\end{FOR}
	\UNTIL{\mbox{$k$ iterations}}\\
	\begin{FOR}{\mbox{each $i$}}
		\begin{IF}{\mbox{$c(p(i)) \le c(i)$ and $c(s(i)) \le c(i)$}}
			\mbox{assign $i$ to the ruling set}
		\end{IF}
	\end{FOR}
	\end{algorithm}
\end{minipage}
\w With appropriate choice of $k$, we can devise an optimal
	$O(\lg n)$-time EREW PRAM algorithm for the list ranking problem.
\eit


\subsection{Tree Contraction}
\bit
\w ({\em{}Expression Evaluation Problem}) Given a parenthesized arithmetic
	expression $E$ (using $+$ and $\cdot$ operations) with values 
	assigned 
	to the variables, evaluate $E$ and all subexpressions of $E$.
\w Note that prefix sums problem is the expression evaluation problem
	on the parenthesized expression
	\[ (\cdots(x_1 + x_2) + x_3 \cdots) + x_n).\]
\w {\em{}Tree contraction\/} is a method of evaluating expression trees
	efficiently in parallel.
\w The {\bf{}SHUNT} operation applied to a leaf in an $n$-left binary tree 
	$T$ results in a {\em{}contracted tree $T'$ in which
	$l$ and $p(l)$ are deleted, and the other
	child $l'$ of $p(l)$ has the parent of $p(l)$ as its parent\/},
	while leaving the relative ordering of the remaining
	leaves unchanged. \vspace{0.2cm}

\begin{minipage}[b]{1cm}
	\begin{algorithm}{TreeContraction}{T}
	\mbox{\tt{}// $T$ is a rooted, ordered, binary $n$-leaf tree}\\
	\mbox{Label the leaves in order from left to right as $1, \cdots, n$}\\
	\begin{FOR}{\ceil{\lg n} \mbox{\ iterations}}
		\mbox{apply SHUNT to all odd-numbered leaves that are 
			left child}\\
		\mbox{apply SHUNT to all odd-numbered leaves that are 
			right child}\\
		\mbox{shift the rightmost bit in the labels of all remaing leaves}
	\end{FOR}
	\end{algorithm}
\end{minipage}
\w Step~2 of {\sf{}TreeContraction} can be implemented using
	{\em{}Euler tour techinque}.
\w The work done in {\bf{}for}-loop is 
	\[O\left(\sum_{i=1}^{\ceil{\lg n}}\frac{n}{2^i}\right) = O(n).\]
\eit

\bibliographystyle{plain}
\bibliography{bib/mac,bib/algo,bib/theory}
\nocite{KR90,Leighton92,Reif92}
\end{document}
